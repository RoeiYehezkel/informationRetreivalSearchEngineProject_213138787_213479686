{"cells":[{"cell_type":"code","execution_count":2,"id":"45802994","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","!pip install -q pyspark"]},{"cell_type":"code","execution_count":3,"id":"4b890c7a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["\n","import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from scipy.sparse import dok_matrix\n","from pyspark.ml.linalg import Vectors\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","import math\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')\n"]},{"cell_type":"code","execution_count":4,"id":"1a84bcec","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar  9 16:20 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["\n","# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*\n"]},{"cell_type":"code","execution_count":5,"id":"05481fb6","metadata":{},"outputs":[],"source":["\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *\n"]},{"cell_type":"code","execution_count":6,"id":"d1a06fe0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/03/09 17:54:28 INFO SparkEnv: Registering MapOutputTracker\n","24/03/09 17:54:28 INFO SparkEnv: Registering BlockManagerMaster\n","24/03/09 17:54:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","24/03/09 17:54:28 INFO SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Initialize a Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"try1\") \\\n","    .config(\"spark.driver.memory\", \"32g\") \\\n","    .config(\"spark.driver.maxResultSize\", \"32g\") \\\n","    .config(\"spark.num.executors\", \"4\") \\\n","    .config(\"spark.executor.memory\", \"5950m\") \\\n","    .config(\"spark.executor.cores\", \"2\") \\\n","    .config(\"spark.executor.instances\", \"4\") \\\n","    .getOrCreate()\n"]},{"cell_type":"code","execution_count":36,"id":"44fc321e","metadata":{},"outputs":[],"source":["spark.stop()"]},{"cell_type":"code","execution_count":7,"id":"6af5cdc5","metadata":{},"outputs":[],"source":["\n","# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'proj_479_187' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name[0] == 'm':\n","        paths.append(full_path+b.name)\n"]},{"cell_type":"code","execution_count":8,"id":"3bb247ce","metadata":{},"outputs":[{"data":{"text/plain":["['gs://proj_479_187/multistream10_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream11_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream11_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream12_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream12_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream13_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream13_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream14_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream14_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream15_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream15_part3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream15_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream16_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream16_part3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream16_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream17_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream17_part3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream17_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream18_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream18_part3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream18_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream19_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream19_part3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream19_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream1_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream20_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream20_part3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream20_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream21_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream21_part3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream21_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream22_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream22_part3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream22_part4_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream22_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream23_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream23_part3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream23_part4_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream23_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream24_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream24_part3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream24_part4_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream24_part5_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream24_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream25_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream25_part3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream25_part4_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream25_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream26_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream27_part2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream27_part3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream27_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream2_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream3_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream4_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream5_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream6_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream7_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream8_preprocessed.parquet',\n"," 'gs://proj_479_187/multistream9_preprocessed.parquet']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["paths"]},{"cell_type":"code","execution_count":9,"id":"a31dcc93","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["\n","parquetFile = spark.read.parquet(*paths)\n","doc_anchor_pairs = parquetFile.select(\"anchor_text\", \"id\").rdd\n","doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n","doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n"]},{"cell_type":"code","execution_count":10,"id":"a70f42c3","metadata":{},"outputs":[],"source":["doc_text_pairs=doc_text_pairs.repartition(104)\n","doc_title_pairs=doc_title_pairs.repartition(104)\n","doc_anchor_pairs=doc_anchor_pairs.repartition(104)"]},{"cell_type":"code","execution_count":11,"id":"90446ece","metadata":{},"outputs":[],"source":["text_splits = doc_text_pairs.randomSplit([1] * 50)"]},{"cell_type":"code","execution_count":12,"id":"187f45f4","metadata":{},"outputs":[{"data":{"text/plain":["PythonRDD[39] at RDD at PythonRDD.scala:53"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":12,"id":"1dd369da","metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Untitled13.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1-96zO3pwOgfY2ST2rhlwseOZ8R8V2qsR\n","\"\"\"\n","\n","import sys\n","from collections import Counter, OrderedDict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","from time import time\n","from pathlib import Path\n","import pickle\n","from google.cloud import storage\n","from collections import defaultdict\n","from contextlib import closing\n","import math\n","\n","PROJECT_ID = 'projectgcp-415908'\n","def get_bucket(bucket_name):\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","\n","def _open(path, mode, bucket=None):\n","    if bucket is None:\n","        return open(path, mode)\n","    return bucket.blob(path).open(mode)\n","\n","# Let's start with a small block size of 30 bytes just to test things out.\n","BLOCK_SIZE = 1999998\n","\n","class MultiFileWriter:\n","    \"\"\" Sequential binary writer to multiple files of up to BLOCK_SIZE each. \"\"\"\n","    def __init__(self, base_dir, name, bucket_name=None):\n","        self._base_dir = Path(base_dir)\n","        self._name = name\n","        self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","        self._file_gen = (_open(str(self._base_dir / f'{name}_{i:03}.bin'),\n","                                'wb', self._bucket)\n","                          for i in itertools.count())\n","        self._f = next(self._file_gen)\n","\n","    def write(self, b):\n","        locs = []\n","        while len(b) > 0:\n","            pos = self._f.tell()\n","            remaining = BLOCK_SIZE - pos\n","            # if the current file is full, close and open a new one.\n","            if remaining == 0:\n","                self._f.close()\n","                self._f = next(self._file_gen)\n","                pos, remaining = 0, BLOCK_SIZE\n","            self._f.write(b[:remaining])\n","            name = self._f.name if hasattr(self._f, 'name') else self._f._blob.name\n","            locs.append((name, pos))\n","            b = b[remaining:]\n","        return locs\n","\n","    def close(self):\n","        self._f.close()\n","\n","class MultiFileReader:\n","    \"\"\" Sequential binary reader of multiple files of up to BLOCK_SIZE each. \"\"\"\n","    def __init__(self, base_dir, bucket_name=None):\n","        self._base_dir = Path(base_dir)\n","        self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","        self._open_files = {}\n","\n","    def read(self, locs, n_bytes):\n","        b = []\n","        for f_name, offset in locs:\n","            f_name = str(self._base_dir / f_name)\n","            if f_name not in self._open_files:\n","                self._open_files[f_name] = _open(f_name, 'rb', self._bucket)\n","            f = self._open_files[f_name]\n","            f.seek(offset)\n","            n_read = min(n_bytes, BLOCK_SIZE - offset)\n","            b.append(f.read(n_read))\n","            n_bytes -= n_read\n","        return b''.join(b)\n","\n","    def close(self):\n","        for f in self._open_files.values():\n","            f.close()\n","\n","    def __exit__(self, exc_type, exc_value, traceback):\n","        self.close()\n","        return False\n","\n","TUPLE_SIZE = 6       # We're going to pack the doc_id and tf values in this\n","                     # many bytes.\n","TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n","\n","\n","class InvertedIndex:\n","    def __init__(self, docs={}):\n","        \"\"\" Initializes the inverted index and add documents to it (if provided).\n","        Parameters:\n","        -----------\n","          docs: dict mapping doc_id to list of tokens\n","        \"\"\"\n","        # stores document frequency per term\n","        self.df = Counter()\n","        # stores total frequency per term\n","        self.term_total = Counter()\n","        # stores posting list per term while building the index (internally),\n","        # otherwise too big to store in memory.\n","        self._posting_list = defaultdict(list)\n","        # mapping a term to posting file locations, which is a list of\n","        # (file_name, offset) pairs. Since posting lists are big we are going to\n","        # write them to disk and just save their location in this list. We are\n","        # using the MultiFileWriter helper class to write fixed-size files and store\n","        # for each term/posting list its list of locations. The offset represents\n","        # the number of bytes from the beginning of the file where the posting list\n","        # starts.\n","        self.posting_locs = defaultdict(list)\n","        self.n = len(docs)\n","        self.docs=list(docs.keys())\n","        self.nf={id:len(text) for id, text in docs.items()}\n","\n","        for doc_id, tokens in docs.items():\n","            self.add_doc(doc_id, tokens)\n","\n","    def calc_tf_idf(self, token, tf):\n","        df = self.df[token]\n","        n = self.n\n","        idf = math.log(n/df)\n","        return tf * idf\n","\n","    def add_doc(self, doc_id, tokens):\n","        \"\"\" Adds a document to the index with a given `doc_id` and tokens. It counts\n","            the tf of tokens, then update the index (in memory, no storage\n","            side-effects).\n","        \"\"\"\n","        w2cnt = Counter(tokens)\n","        self.term_total.update(w2cnt)\n","        for w, cnt in w2cnt.items():\n","            self.df[w] = self.df.get(w, 0) + 1\n","            tf_idf = self.calc_tf_idf(w, cnt)\n","            self._posting_list[w].append((doc_id, tf_idf))\n","\n","\n","    def write_index(self, base_dir, name, bucket_name=None):\n","        \"\"\" Write the in-memory index to disk. Results in the file:\n","            (1) `name`.pkl containing the global term stats (e.g. df).\n","        \"\"\"\n","        #### GLOBAL DICTIONARIES ####\n","        self._write_globals(base_dir, name, bucket_name)\n","\n","    def _write_globals(self, base_dir, name, bucket_name):\n","        path = str(Path(base_dir) / f'{name}.pkl')\n","        bucket = None if bucket_name is None else get_bucket(bucket_name)\n","        with _open(path, 'wb', bucket) as f:\n","            pickle.dump(self, f)\n","\n","    def __getstate__(self):\n","        \"\"\" Modify how the object is pickled by removing the internal posting lists\n","            from the object's state dictionary.\n","        \"\"\"\n","        state = self.__dict__.copy()\n","        del state['_posting_list']\n","        return state\n","\n","    def posting_lists_iter(self, base_dir, bucket_name=None):\n","        \"\"\" A generator that reads one posting list from disk and yields\n","            a (word:str, [(doc_id:int, tf:int), ...]) tuple.\n","        \"\"\"\n","        with closing(MultiFileReader(base_dir, bucket_name)) as reader:\n","            for w, locs in self.posting_locs.items():\n","                b = reader.read(locs, self.df[w] * TUPLE_SIZE)\n","                posting_list = []\n","                for i in range(self.df[w]):\n","                    doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","                    tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","                    posting_list.append((doc_id, tf))\n","                yield w, posting_list\n","\n","    def read_a_posting_list(self, base_dir, w, bucket_name=None):\n","        posting_list = []\n","        if not w in self.posting_locs:\n","            return posting_list\n","        with closing(MultiFileReader(base_dir, bucket_name)) as reader:\n","            locs = self.posting_locs[w]\n","            b = reader.read(locs, self.df[w] * TUPLE_SIZE)\n","            for i in range(self.df[w]):\n","                doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","                tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","                posting_list.append((doc_id, tf))\n","        return posting_list\n","\n","    @staticmethod\n","    def write_a_posting_list(b_w_pl, base_dir, bucket_name=None):\n","        posting_locs = defaultdict(list)\n","        bucket_id, list_w_pl = b_w_pl\n","\n","        with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","            for w, pl in list_w_pl:\n","                # convert to bytes\n","                b = b''.join([struct.pack('>If', doc_id, tf) for doc_id, tf in pl])\n","\n","                # write to file(s)\n","                locs = writer.write(b)\n","                # save file locations to index\n","                posting_locs[w].extend(locs)\n","            path = str(Path(base_dir) / f'{bucket_id}_posting_locs.pickle')\n","            bucket = None if bucket_name is None else get_bucket(bucket_name)\n","            with _open(path, 'wb', bucket) as f:\n","                pickle.dump(posting_locs, f)\n","        return bucket_id\n","\n","\n","    @staticmethod\n","    def read_index(base_dir, name, bucket_name=None):\n","        path = str(Path(base_dir) / f'{name}.pkl')\n","        bucket = None if bucket_name is None else get_bucket(bucket_name)\n","        with _open(path, 'rb', bucket) as f:\n","            return pickle.load(f)"]},{"cell_type":"code","execution_count":17,"id":"2760f931","metadata":{},"outputs":[],"source":["import struct\n","\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){0,24}\"\"\", re.UNICODE)\n","def tokenize(text):\n","    stemmer = PorterStemmer()\n","    tokens = [stemmer.stem(token.group()) for token in RE_WORD.finditer(text.lower())]\n","    return [token for token in tokens if token not in all_stopwords]\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","def word_count(text, id):\n","    ''' Count the frequency of each word in `text` (tf) that is not included in\n","    `all_stopwords` and return entries that will go into our posting lists.\n","    Parameters:\n","    -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","    Returns:\n","    --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs\n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","    '''\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    stemmer = PorterStemmer()\n","    for token in tokens:\n","        token=stemmer.stem(token)\n","\n","\n","    count = Counter([token for token in tokens if token not in all_stopwords])\n","    if len(count)==0:\n","        count = Counter([token for token in tokens])\n","    res = []\n","    for tup in count.items():\n","        res.append((stemmer.stem(tup[0]),(id,tup[1])))\n","    return res\n","\n","def reduce_word_counts(unsorted_pl):\n","    ''' Returns a sorted posting list by wiki_id.\n","    Parameters:\n","    -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples\n","    Returns:\n","    --------\n","    list of tuples\n","      A sorted posting list.\n","    '''\n","    # YOUR CODE HERE\n","    sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])\n","    return sorted_pl\n","\n","def calculate_df(postings):\n","    ''' Takes a posting list RDD and calculate the df for each token.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","    '''\n","    # YOUR CODE HERE\n","    return postings.map(lambda x: (x[0], len(x[1])))\n","\n","from math import log\n","\n","def calculate_tf_idf(postings, total_docs):\n","    ''' Calculates the TF-IDF for each token.\n","    Parameters:\n","    -----------\n","      postings: RDD\n","        An RDD where each element is a (token, postings_list) pair, and\n","        postings_list is a list of (document_id, term_frequency) tuples for that token.\n","      total_docs: int\n","        The total number of documents in the corpus.\n","    Returns:\n","    --------\n","      RDD\n","        An RDD where each element is a (token, [(document_id, tf_idf), ...]) pair.\n","    '''\n","    # Calculate IDF for each token\n","    # Note: log base is e by default, which is common in TF-IDF calculations\n","    def idf(df):\n","        return log(total_docs / df)\n","    \n","    # Calculate TF-IDF for each (document_id, term_frequency) tuple in the postings list\n","    def tf_idf(postings_list):\n","        df = len(postings_list)\n","        return [(doc_id, tf * idf(df)) for doc_id, tf in postings_list]\n","    \n","    return postings.map(lambda x: (x[0], tf_idf(x[1])))\n","\n","def partition_postings_and_write(postings):\n","    ''' A function that partitions the posting lists into buckets, writes out\n","    all posting lists in a bucket to disk, and returns the posting locations for\n","    each bucket. Partitioning should be done through the use of `token2bucket`\n","    above. Writing to disk should use the function  `write_a_posting_list`, a\n","    static method implemented in inverted_index_colab.py under the InvertedIndex\n","    class.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and\n","      offsets its posting list was written to. See `write_a_posting_list` for\n","      more details.\n","    '''\n","    # YOUR CODE HERE\n","    buckets = postings.map(lambda x: (token2bucket_id(x[0]),(x[0],x[1]))).groupByKey().mapValues(list)\n","    res = buckets.map(lambda x: InvertedIndex().write_a_posting_list(x,\"text/\", bucket_name))\n","    return res\n","\n","def partition_postings_and_write_title(postings):\n","    ''' A function that partitions the posting lists into buckets, writes out\n","    all posting lists in a bucket to disk, and returns the posting locations for\n","    each bucket. Partitioning should be done through the use of `token2bucket`\n","    above. Writing to disk should use the function  `write_a_posting_list`, a\n","    static method implemented in inverted_index_colab.py under the InvertedIndex\n","    class.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and\n","      offsets its posting list was written to. See `write_a_posting_list` for\n","      more details.\n","    '''\n","    # YOUR CODE HERE\n","    buckets = postings.map(lambda x: (token2bucket_id(x[0]),(x[0],x[1]))).groupByKey().mapValues(list)\n","    res = buckets.map(lambda x: InvertedIndex().write_a_posting_list(x, \"title/\",bucket_name))\n","    return res"]},{"cell_type":"code","execution_count":20,"id":"38061f7f","metadata":{},"outputs":[],"source":["def group_by_b_id(items):\n","    grouped_dict = {}\n","    for b_id, item in items:\n","        if b_id not in grouped_dict:\n","            grouped_dict[b_id] = []\n","        grouped_dict[b_id].append(item)\n","    return [(b_id, grouped_dict[b_id]) for b_id in grouped_dict]\n","\n","\n"]},{"cell_type":"code","execution_count":14,"id":"4f4ff389","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["total_docs=doc_title_pairs.count()"]},{"cell_type":"code","execution_count":55,"id":"b3cb700c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["word_counts_title = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","title2df = calculate_tf_idf(postings_title, total_docs)\n","title2df_dict = title2df.collectAsMap()\n","_ = partition_postings_and_write_title(title2df).collect()"]},{"cell_type":"code","execution_count":24,"id":"bcbe434d","metadata":{},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='title'):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":26,"id":"f1a1407b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_title.pkl\r\n"]}],"source":["# Create inverted index instance\n","inverted_title = InvertedIndex(title2df_dict)\n","# Adding the posting locations dictionary to the inverted index\n","inverted_title.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted_title.df = title2df_dict\n","# write the global stats out\n","inverted_title.write_index('.', 'index_title', bucket_name)\n","# upload to gs\n","index_src = \"index_title.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"3fa896bd","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_0.pkl\n","0 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_1.pkl\n","1 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_2.pkl\n","2 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_3.pkl\n","3 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_4.pkl\n","4 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_5.pkl\n","5 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_6.pkl\n","6 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_7.pkl\n","7 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_8.pkl\n","8 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_9.pkl\n","9 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_10.pkl\n","10 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_11.pkl\n","11 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_12.pkl\n","12 is done\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CommandException: No URLs matched: index_text_part_13.pkl\n","13 is done\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 114:=========================>                            (50 + 8) / 104]\r"]}],"source":["for index, rdd in enumerate(text_splits):\n","    word_counts = rdd.flatMap(lambda x: word_count(x[0], x[1]))\n","    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","    postings_filtered = postings.filter(lambda x: len(x[1])>70)\n","    # filtering postings and calculate df\n","    text2df = calculate_tf_idf(postings_filtered, total_docs)\n","    text2df_dict = text2df.collectAsMap()\n","    _ = partition_postings_and_write(text2df).collect()\n","    # collect all posting lists locations into one super-set\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix='title'):\n","        if not blob.name.endswith(\"pickle\"):\n","            continue\n","        with blob.open(\"rb\") as f:\n","            posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs[k].extend(v)\n","    # Create inverted index instance\n","    inverted_text = InvertedIndex()\n","    # Adding the posting locations dictionary to the inverted index\n","    inverted_text.posting_locs = super_posting_locs\n","    # Add the token - df dictionary to the inverted index\n","    inverted_text.df = text2df_dict\n","    # write the global stats out\n","    inverted_text.write_index('.', f'index_text_part_{index}', bucket_name)\n","    # upload to gs\n","    index_src = f\"index_text_part_{index}.pkl\"\n","    index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n","    !gsutil cp $index_src $index_dst\n","    print(f'{index} is done')"]},{"cell_type":"code","execution_count":45,"id":"b9fb2a38","metadata":{},"outputs":[],"source":["word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","postings_filtered = postings.filter(lambda x: len(x[1])>70)\n","w2df_part = calculate_tf_idf(postings_filtered, total_docs)\n","_ = partition_postings_and_write(w2df_part)"]},{"cell_type":"code","execution_count":47,"id":"7c430896","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Exception in thread \"serve RDD 43\" java.net.SocketTimeoutException: Accept timed out\n","\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n","\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n","\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n","\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n","                                                                                \r"]}],"source":["partitioned_dct = w2df_part.mapPartitions(lambda iterator: dict(iterator)).collect()"]},{"cell_type":"code","execution_count":54,"id":"64227ff8","metadata":{},"outputs":[{"data":{"text/plain":["['cristian',\n"," 'dörpfeld',\n"," 'cookei',\n"," 'insomuch',\n"," \"cheese'\",\n"," 'lučko',\n"," 'atraco',\n"," 'ithamar',\n"," '1249',\n"," 'vibia',\n"," 'crosswhit',\n"," 'jonk',\n"," 'zwettl',\n"," 'boudot',\n"," 'marchon',\n"," '19541',\n"," 'diver',\n"," 'vo',\n"," 'sirohi',\n"," 'hoyland',\n"," 'subaltern',\n"," 'schine',\n"," 'emigdio',\n"," \"spear'\",\n"," 'polinski',\n"," 'renaldi',\n"," 'lajunen',\n"," 'montyon',\n"," 'insiem',\n"," 'libertador',\n"," 'sarva',\n"," 'bonchurch',\n"," 'paralyz',\n"," 'kunsthistorisch',\n"," '9-e',\n"," 'gap-fil',\n"," 'wemp',\n"," 'bouley',\n"," 'waart',\n"," 'miyaguchi',\n"," 'schizoaffect',\n"," 'matanović',\n"," 'ōura',\n"," 'frottag',\n"," 'grandier',\n"," 'remors',\n"," 'fail-saf',\n"," 'mcgee',\n"," 'oolong',\n"," '1-meter',\n"," 'stigliano',\n"," 'beckley',\n"," 'jombo',\n"," 'kumit',\n"," 'ន',\n"," 'forestay',\n"," 'shredder',\n"," 'swelter',\n"," 'pleasenc',\n"," 'kuko',\n"," 'oceanica',\n"," 'gammaridea',\n"," '3954',\n"," 'suppiluliuma',\n"," 'd51',\n"," 'willowi',\n"," \"bolton'\",\n"," 'suntech',\n"," 'tzedakah',\n"," 'wherefor',\n"," \"d'andr\",\n"," 'tether',\n"," 'valedictorian',\n"," 'frome',\n"," \"gaozong'\",\n"," 'digital-to-analog',\n"," 'kukë',\n"," 'gilla',\n"," '40-day',\n"," 'wringer',\n"," 'public-interest',\n"," \"o'mahony'\",\n"," 'supachai',\n"," 'somin',\n"," 'twentyman',\n"," '199110',\n"," '1968-1974',\n"," 'hongyun',\n"," 'balaram',\n"," 'tañón',\n"," 'chichest',\n"," 'non-north',\n"," 'perk',\n"," 'filker',\n"," '23may',\n"," 'perspex',\n"," 'horologist',\n"," 'feher',\n"," '16-30',\n"," 'misalloc',\n"," 'saint-benoît',\n"," \"liechtenstein'\",\n"," 'sicher',\n"," 'yefimovich',\n"," 'rhizobia',\n"," 'ultraseven',\n"," 'breath-hold',\n"," 'bantwal',\n"," \"euripides'\",\n"," '6338',\n"," 'glomar',\n"," 'ertan',\n"," 'alien',\n"," 'kv',\n"," 'panya',\n"," 'suc',\n"," 'reengin',\n"," 'abdil',\n"," 'septemberheart',\n"," '84-86',\n"," 'peakreach',\n"," 'psp',\n"," 'mötley',\n"," 'teletypewrit',\n"," 'komárom-esztergom',\n"," 'sturani',\n"," 'raoc',\n"," 'temagami',\n"," 'timmermann',\n"," 'caponi',\n"," 'trick-or-treat',\n"," 'four-year-old',\n"," 'mesabi',\n"," 'surli',\n"," 'sino-u',\n"," 'on-stat',\n"," 'resentenc',\n"," \"d'errico\",\n"," 'cornaredo',\n"," 'zigong',\n"," 'keppi',\n"," \"6'\",\n"," 'wegen',\n"," 'ugonna',\n"," 'southold',\n"," 'darki',\n"," 'action-platform',\n"," 'korwin-mikk',\n"," 'dar24',\n"," 'raupp',\n"," 'croiset',\n"," 'soarin',\n"," 'rishiri',\n"," 'exasperata',\n"," 'reduit',\n"," 'centropoli',\n"," 'sbl',\n"," 'steilacoom',\n"," 'blankenes',\n"," 'yerxa',\n"," 'khidr',\n"," 'swapper',\n"," 'degrad',\n"," 'runaway',\n"," 'kristiansund',\n"," 'haydon',\n"," 'nyota',\n"," 'amidah',\n"," 'resia',\n"," 'kurram',\n"," 'mcskim',\n"," 'geingob',\n"," 'x20px',\n"," 'koltai',\n"," 'vilardi',\n"," \"brecht'\",\n"," 'trixi',\n"," 'schriften',\n"," 'disquisit',\n"," 'batty',\n"," 'spinig',\n"," \"burnside'\",\n"," 'madrina',\n"," 'ajj',\n"," 'brubak',\n"," 'tevin',\n"," \"urban'\",\n"," 'ger8',\n"," 'carchi',\n"," 'hagner',\n"," 'bisphosphon',\n"," 'silverwat',\n"," 'schmalkalden-meiningen',\n"," 'halberdi',\n"," 'kremmen',\n"," 'grič',\n"," 'yakisoba',\n"," 'benyahia',\n"," 'shefki',\n"," 'kfmb-tv',\n"," 'suay',\n"," 'f-106a',\n"," '9761',\n"," 'skycran',\n"," 'tenjho',\n"," 'emg',\n"," 'millpond',\n"," 'portray',\n"," 'sligo',\n"," 'racehors',\n"," 'glorious',\n"," 'major-gener',\n"," 'methemoglobinemia',\n"," 'halfpenni',\n"," '#246',\n"," 'gesamtschul',\n"," 'vengeanc',\n"," 'tilli',\n"," 'dealloc',\n"," 'cathay',\n"," '0new',\n"," 'darvish',\n"," \"d'orbigni\",\n"," 'kabataan',\n"," 'cindric',\n"," 'alexandrinski',\n"," \"morley'\",\n"," 'iwgp',\n"," 'unworthi',\n"," 'truu',\n"," 'pedantri',\n"," 'sirou',\n"," 'macrobiu',\n"," 'magneti',\n"," '05premier',\n"," 'mid-1981',\n"," 'yendi',\n"," 'heydari',\n"," 'dommag',\n"," 'bhrt',\n"," 'all-zero',\n"," 'phlaeothripida',\n"," 'gurdeep',\n"," 'unexpress',\n"," 'mentali',\n"," 'guel',\n"," 'fathead',\n"," \"gavin'\",\n"," 'carus-verlag',\n"," 'pontebba',\n"," '8502',\n"," '10021',\n"," 'jae-jin',\n"," 'vazhga',\n"," 'german-soviet',\n"," 'keiper',\n"," 'maeva',\n"," 'gletscher',\n"," 'paloalto',\n"," 'borgman',\n"," 'kosovo-metohija',\n"," 'tuân',\n"," 'bcbg',\n"," 'obadia',\n"," 'méricourt',\n"," 'e-werk',\n"," 're-divid',\n"," 'cath',\n"," 'murdoch',\n"," 'slovenian',\n"," 'espagn',\n"," 'tovey',\n"," 'arghun',\n"," 'פ',\n"," 'second-deadliest',\n"," 'dupuytren',\n"," 'tshogpa',\n"," '8-gun',\n"," \"blackman'\",\n"," 'cay',\n"," 'dewart',\n"," 'super-comb',\n"," \"franchise'\",\n"," 'east-flow',\n"," 'transcaspian',\n"," 'pennisetum',\n"," 'year-book',\n"," '4462',\n"," 'task',\n"," 'pale',\n"," 'groundcov',\n"," 'volubl',\n"," '23-17',\n"," 'schefter',\n"," 'bacigalupi',\n"," 'gowtham',\n"," 'fisch',\n"," 'flere',\n"," 'wia',\n"," 'craigieburn',\n"," 'riyal',\n"," 'cleugh',\n"," '60-70',\n"," '19975',\n"," 'linichuk',\n"," 'yukich',\n"," 'fyer',\n"," 'gwiazdami',\n"," 'lyset',\n"," 'nicolli',\n"," 'oileán',\n"," 'financiero',\n"," 'rufous-neck',\n"," 'whinni',\n"," 'naukova',\n"," 'hortulana',\n"," 'headiesbest',\n"," 'camaleón',\n"," 'loughinisland',\n"," 'coin-toss',\n"," 'mesoplodon',\n"," 'anina',\n"," 'aagey',\n"," 'agrícola',\n"," 'visu',\n"," '3q',\n"," 'gambon',\n"," \"robb'\",\n"," 'buyid',\n"," 'ingam',\n"," 'microgenr',\n"," 'listserv',\n"," 'rodham',\n"," 'alshon',\n"," 'night-bloom',\n"," 'senec',\n"," '0259',\n"," 'geass',\n"," 'stadium',\n"," \"darwin'\",\n"," 'telugu-languag',\n"," 'teamnatfromtorecordgwdlwi',\n"," 'counter-intuit',\n"," 'śródmieści',\n"," 'tiburón',\n"," 'sušica',\n"," 'apolinari',\n"," 'b-list',\n"," '50-100',\n"," 'borain',\n"," 'qián',\n"," 'danceu',\n"," '720',\n"," 'midrash',\n"," 'denham',\n"," '151st',\n"," 'nait',\n"," 'blockchain-bas',\n"," 'yod',\n"," 'higher-resolut',\n"," 'balsfjord',\n"," 'nearctica',\n"," \"rákóczi'\",\n"," 'beccafumi',\n"," 'amazilia',\n"," 'yinxu',\n"," 'tisan',\n"," 'immunostain',\n"," 'précieux',\n"," 'bt-7',\n"," \"valkyrie'\",\n"," 'bernar',\n"," \"pulp'\",\n"," 'hondo',\n"," 'gbd',\n"," 'bastin',\n"," 'bondoc',\n"," 'proto-dravidian',\n"," 'koru',\n"," 'saraj',\n"," '6-volt',\n"," \"ya'an\",\n"," '31-26',\n"," \"stevens'\",\n"," 'trotha',\n"," 'm3a1',\n"," 'schepisi',\n"," 'ever',\n"," 'pontiac',\n"," 'thorvald',\n"," 'snodgrass',\n"," 'returne',\n"," 'spectro',\n"," 'jean-vinc',\n"," 'sharrow',\n"," 'mallia',\n"," 'tutu',\n"," 'poiana',\n"," \"l'associ\",\n"," 'jean-guy',\n"," 'fuster',\n"," '78rpm',\n"," '8251',\n"," 'datin',\n"," 'salticida',\n"," 'allocasuarina',\n"," 'oyo',\n"," 'kadalundi',\n"," 'balabhadra',\n"," 'clémenti',\n"," 'pre-1920',\n"," 'taiheiyo',\n"," 'nagwa',\n"," 'gif-sur-yvett',\n"," 'centralis',\n"," \"cauchy'\",\n"," '#612',\n"," 'sarolta',\n"," 'subhankar',\n"," 'foregon',\n"," 'schafheitlin',\n"," 'warbrick',\n"," 'thrust-to-weight',\n"," 'europei',\n"," \"stereogum'\",\n"," 'non-automat',\n"," 'pašanski',\n"," 'fdlr',\n"," 'neuromyel',\n"," 'mch36',\n"," 'reopen',\n"," 'gasif',\n"," '2211',\n"," '8733',\n"," 'minn',\n"," 'mpm',\n"," 'nashua',\n"," 'brunnea',\n"," 'mcgrigor',\n"," 'chukwu',\n"," 'gangut',\n"," 'seats2001',\n"," 'marsal',\n"," 'hellebaut',\n"," '199x199px',\n"," 'uebersicht',\n"," 'nunnal',\n"," 'rhea',\n"," '40-year',\n"," 'esp3',\n"," 'overemphasi',\n"," 'at-sea',\n"," 'rajida',\n"," 'dmitrov',\n"," 'inter-villag',\n"," 'dictorum',\n"," 'beatminerz',\n"," 'a-1',\n"," 'ymir',\n"," 'mispronunci',\n"," 'gasoline-pow',\n"," 'palembang',\n"," 'tartou',\n"," 'juniperu',\n"," 'kratzmann',\n"," 'citerior',\n"," 'phya',\n"," \"mistress'\",\n"," 'parlier',\n"," 'mauss',\n"," \"morsi'\",\n"," 'vergangen',\n"," 'candler',\n"," 'vyjayanthimala',\n"," 'laulala',\n"," 'romer',\n"," 'recolonis',\n"," 'zenga',\n"," 'nayer',\n"," 'heredero',\n"," 'invest',\n"," 'ilustrado',\n"," 'krivoy',\n"," 'ferrini',\n"," 'odumas',\n"," 'mycosphaerella',\n"," 'padovani',\n"," 'multic',\n"," 'nov-1989',\n"," 'avdija',\n"," 'marselisborg',\n"," 'chasidim',\n"," 'kherlen',\n"," 'sangamon',\n"," 'shep',\n"," 'all-arena',\n"," 'sarangi',\n"," '2940',\n"," '1scoreteam',\n"," 'spiridonova',\n"," 'structureless',\n"," 'reculé',\n"," 'sandøi',\n"," 'unaccustom',\n"," 'tangl',\n"," 'america-exclus',\n"," 'sieber',\n"," \"haven'\",\n"," 'firesign',\n"," 'board-gam',\n"," 'llc',\n"," 'woden',\n"," \"mccall'\",\n"," 'fanti',\n"," 'hatha',\n"," 'wein',\n"," 'brainiac',\n"," 'perceptor',\n"," 'nizam',\n"," 'zongren',\n"," 'sedgefield',\n"," 'addax',\n"," 'ladakhi',\n"," 'dimostheni',\n"," 'ximénez',\n"," 'shuar',\n"," 'jerrold',\n"," 'scw',\n"," 'dalia',\n"," 'cheney',\n"," 'dong-hyuk',\n"," 'waktu',\n"," 'juncker',\n"," 'lesean',\n"," 'rudl',\n"," 'creggan',\n"," \"ninja'\",\n"," 'min-seo',\n"," 'dargaz',\n"," 'lenhardt',\n"," '2-gun',\n"," 'sargentii',\n"," 'mokrani',\n"," 'hukawng',\n"," 'lamelo',\n"," 'patai',\n"," 'kd7',\n"," 'abib',\n"," 'volksschul',\n"," 'ucl',\n"," 'cia',\n"," 'gallega',\n"," 'shojiro',\n"," 'n16',\n"," 'kunoichi',\n"," 'inventio',\n"," 'yūhi',\n"," 'manha',\n"," '5569',\n"," '4499',\n"," 'doveri',\n"," 'wölfe',\n"," 'jehoiachin',\n"," 'bucatinski',\n"," 'cyre',\n"," 'kostelec',\n"," 'mahal',\n"," 'tissa',\n"," 'brokaw',\n"," 'canac',\n"," 'ies',\n"," 'houchen',\n"," 'greenstreet',\n"," 're-conceptu',\n"," 'whitefac',\n"," 'akbarpur',\n"," 'mitscher',\n"," 'sangiovannes',\n"," '128kb',\n"," 'gastoni',\n"," 'andrenida',\n"," 'r-la',\n"," 'euronew',\n"," '7152',\n"," 'hym',\n"," 'northman',\n"," '8048',\n"," 'mondt',\n"," 'rodna',\n"," 'ktr',\n"," '8072',\n"," 'olimpo',\n"," 'ośrodek',\n"," 'siller',\n"," 'polyana',\n"," 'costipenni',\n"," 'prematho',\n"," 'kakamigahara',\n"," 'gitli',\n"," 'anti-taliban',\n"," 'mercedescup',\n"," 'seribu',\n"," 'reba',\n"," 'input',\n"," 'templetown',\n"," 'impôt',\n"," 'radik',\n"," 'petrucci',\n"," 'albizu',\n"," '6395',\n"," 'baboquivari',\n"," 'wgt',\n"," 'frc',\n"," 'malaya',\n"," 'tearoom',\n"," 'aeolid',\n"," 'bassinet',\n"," '5784',\n"," 'same-day',\n"," 'mbyte',\n"," 'thuan',\n"," 'kace',\n"," 'aleksandrova',\n"," 'kieler',\n"," 'congenit',\n"," 'sumida',\n"," 'gramma',\n"," 'ex-u',\n"," 'bayli',\n"," 'porton',\n"," 'byaverag',\n"," 'khairat',\n"," 'week5',\n"," 'davoren',\n"," 'giroud',\n"," 'tofan',\n"," 'karlsberg',\n"," 'vestita',\n"," 'chū',\n"," 'tétouan',\n"," 'colebatch',\n"," '4-23',\n"," 'desaguli',\n"," 'siad',\n"," 'küttel',\n"," 'kaydet',\n"," '19424',\n"," 'kazinski',\n"," 'agstafa',\n"," 'glenholm',\n"," 'legislativa',\n"," 'shivaay',\n"," \"división'\",\n"," '5904',\n"," 'trabucco',\n"," 'verla',\n"," 'norito',\n"," 'd-texa',\n"," 'cutoff',\n"," '6461',\n"," 'aloha',\n"," 'budworth',\n"," 'joniški',\n"," '5090',\n"," 'kannukkul',\n"," 'grimké',\n"," '5757',\n"," 'mantrap',\n"," 'helplessli',\n"," '07ligu',\n"," 'tennakoon',\n"," 'sanskruti',\n"," 'benincasa',\n"," 'nuncio',\n"," 'schmadel',\n"," 'şükrü',\n"," 'sidetrack',\n"," 'aubigni',\n"," 'alassio',\n"," 'youichi',\n"," 'charilao',\n"," 'ণ',\n"," 'balikh',\n"," 'michôd',\n"," 'oregonensi',\n"," 'olavsson',\n"," 'verkhoyansk',\n"," 'kaaki',\n"," 'marchegiani',\n"," 'winster',\n"," 'fardel',\n"," 'tal1',\n"," 'takato',\n"," 'middle-s',\n"," 'medano',\n"," 'multiplay',\n"," \"a'\",\n"," 'stryper',\n"," 'hoteli',\n"," 'pro-russian',\n"," 'dijkgraaf',\n"," 'tunb',\n"," 'soji',\n"," '42012',\n"," '5384',\n"," 'okcupid',\n"," 'maculicolli',\n"," 'kingmoor',\n"," 'ueckermünd',\n"," 'road3',\n"," 'escherich',\n"," 'adex',\n"," 'eighty-four',\n"," 'seinem',\n"," 'chromat',\n"," 'lanci',\n"," 'honorum',\n"," 'beloy',\n"," 'hajaco',\n"," 'smurfett',\n"," 'djia',\n"," '4704',\n"," \"varley'\",\n"," 'dare',\n"," 'ftv',\n"," 'framu',\n"," 'innocenti',\n"," 'five-volum',\n"," 'grondin',\n"," 'karpal',\n"," 'footstep',\n"," 'taiko',\n"," 'sre',\n"," 'dobkin',\n"," 'priozerski',\n"," 'lamot',\n"," 'kudin',\n"," 'autonomedia',\n"," 'nakajima',\n"," 'admiss',\n"," 'lakeba',\n"," 'manoli',\n"," 'prism',\n"," 'cherkasova',\n"," 'willmar',\n"," 'agolli',\n"," 'velada',\n"," '100-120',\n"," 'knuckledust',\n"," 'odihr',\n"," 'jerel',\n"," 'step-through',\n"," 'utian',\n"," \"cipher'\",\n"," 'kébé',\n"," 'mollissima',\n"," 'rung',\n"," 'sokoloff',\n"," 'gat',\n"," 'tae-young',\n"," 'hitech',\n"," 'setsu',\n"," 'weilburg',\n"," 'transversu',\n"," 'erlinda',\n"," 'linotyp',\n"," 'luminos',\n"," 'basti',\n"," 'globaltwitch',\n"," \"arsenal'\",\n"," 'zsanett',\n"," 'nhm',\n"," 'andersonii',\n"," 'concilium',\n"," 'renfrewshir',\n"," 'bhim',\n"," 'outcomeno',\n"," 'floresta',\n"," 'elmshorn',\n"," 'baptistri',\n"," 'arrogantli',\n"," 'incerti',\n"," 'brycheiniog',\n"," 'engli',\n"," 'pota',\n"," 'lindstrøm',\n"," 'afric',\n"," 'maoh',\n"," 'pazhani',\n"," 'vasool',\n"," 'strychno',\n"," 'godward',\n"," 'flw',\n"," 'も',\n"," 'germany2',\n"," 'party18',\n"," 'hagfish',\n"," 'wibberley',\n"," 'striven',\n"," 'rostropovich',\n"," 'calado',\n"," 'montilla',\n"," '82005',\n"," 'yarnel',\n"," '18april',\n"," 'ghirlandaio',\n"," 'chorleywood',\n"," 'sundman',\n"," 'byfield',\n"," 'tkr',\n"," 'muqaddam',\n"," '16loctob',\n"," 'hateley',\n"," 'khwaeng',\n"," 'fundoshi',\n"," 'saale-holzland',\n"," 'singu',\n"," 'stic',\n"," 'rhacophorida',\n"," 'four-over-four',\n"," 'jabe',\n"," 'sailendra',\n"," 'al-shabab',\n"," 'newly-form',\n"," 'zosima',\n"," 'non-indian',\n"," 'groult',\n"," 'alô',\n"," 'katsuhito',\n"," 'stratolin',\n"," 'fazil',\n"," \"gaye'\",\n"," 'jahović',\n"," 'natrix',\n"," 'rendra',\n"," 'heart-break',\n"," 'montelibano',\n"," 'redbox',\n"," 'start',\n"," 'bha',\n"," '4788',\n"," 'hersch',\n"," 'córdova',\n"," 'longship',\n"," 'hollick',\n"," 'clouser',\n"," '76-yard',\n"," 'yordani',\n"," 'pandionida',\n"," \"oakley'\",\n"," 'tridentatu',\n"," 'one-form',\n"," \"hackney'\",\n"," 'ron',\n"," 'gracilior',\n"," 'four-and-a-half',\n"," 'आत',\n"," '2na',\n"," 'criado',\n"," 'nữ',\n"," '24-23',\n"," 'stop-chamf',\n"," 'nacw',\n"," 'natavan',\n"," 'joppa',\n"," 'aqualung',\n"," 'verschoor',\n"," 'gouvern',\n"," 'zsasz',\n"," 'andronikashvili',\n"," 'gullan',\n"," '6315',\n"," 'degtyarev',\n"," 'gripsholm',\n"," 'shix',\n"," 'hyphomicrobial',\n"," 'rasher',\n"," 'side-fold',\n"," 'ridgeway',\n"," 'saleem',\n"," 'volstead',\n"," 'silan',\n"," 'rams',\n"," 'merten',\n"," 'purba',\n"," 'fornix',\n"," 'tree-plant',\n"," 'min-jeong',\n"," \"servicemen'\",\n"," 'takaji',\n"," '9933',\n"," 'noyc',\n"," 'glazebrook',\n"," '6074',\n"," 'touristi',\n"," 'blockhau',\n"," 'devon',\n"," 'anesthet',\n"," 'masku',\n"," \"atatürk'\",\n"," 'lightvessel',\n"," '3372',\n"," 'summorum',\n"," 'sicard',\n"," 'campiglia',\n"," 'alvito',\n"," 'tizzard',\n"," '8156',\n"," 'bertrando',\n"," 'durando',\n"," 'júlíusson',\n"," '1000x',\n"," 'hegley',\n"," 'narol',\n"," 'croz',\n"," 'front-lin',\n"," 'gonâv',\n"," 'schalli',\n"," 'meistersing',\n"," 'giusto',\n"," \"tailor'\",\n"," 'erden',\n"," \"enzo'\",\n"," '8680',\n"," 'leadership',\n"," 'up-to-d',\n"," 'vardan',\n"," 'lufthansa',\n"," 'helmet-mount',\n"," 'jong-su',\n"," 'nuss',\n"," 'foil',\n"," 'johansson',\n"," 'single-parti',\n"," 'breakneck',\n"," 'aula',\n"," 'gittin',\n"," 's38',\n"," 'broo',\n"," 'trichardt',\n"," 'silspr',\n"," 'greasi',\n"," 'ett',\n"," 'gavriel',\n"," 'huberman',\n"," 'hamiltonian',\n"," '22septemb',\n"," 'sixty-day',\n"," 'barnstead',\n"," '18win18',\n"," 'kilmuir',\n"," 'gandha',\n"," 'kronen',\n"," 'well-match',\n"," 'forefeet',\n"," 'arag',\n"," 'surfboat',\n"," \"mongkut'\",\n"," 'raymor',\n"," 'ready-to-fli',\n"," 'banch',\n"," 'dinotopia',\n"," 'teamattend',\n"," 'tagma',\n"," 'edvina',\n"," 'commission',\n"," '¾',\n"," 'kotak',\n"," 'manati',\n"," 'bean',\n"," 'thackervil',\n"," 'celma',\n"," 'championshipsla',\n"," 'janovitz',\n"," 'bródi',\n"," 'arrowood',\n"," 'straubing-bogen',\n"," 'kensico',\n"," 'encyclopédi',\n"," 'atwal',\n"," 'dalila',\n"," 'geonim',\n"," 'tetralog',\n"," 'flanagan',\n"," 'despacito',\n"," 'trewava',\n"," 'rader',\n"," 'banham',\n"," 'ferrard',\n"," 'biopesticid',\n"," 'vyazniki',\n"," '3551',\n"," 'venantiu',\n"," 'olivet',\n"," 'firat',\n"," 'scole',\n"," 'kpt',\n"," 'leukerbad',\n"," 'sanguinaria',\n"," 'inas',\n"," ...]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["partitioned_dct"]},{"cell_type":"code","execution_count":null,"id":"3325a5b7","metadata":{},"outputs":[],"source":["for idx, dct in enumerate(partitioned_dct):\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix=f\"text/part_{idx}\"):\n","        if not blob.name.endswith(\"pickle\"):\n","            continue\n","        with blob.open(\"rb\") as f:\n","            posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs[k].extend(v)\n","    # Create inverted index instance\n","    inverted_text = InvertedIndex()\n","    # Adding the posting locations dictionary to the inverted index\n","    inverted_text.posting_locs = super_posting_locs\n","    # Add the token - df dictionary to the inverted index\n","    inverted_text.df = dct\n","    # write the global stats out\n","    inverted_title.write_index('.', f'index_text_part_{idx}', bucket_name)\n","    # upload to gs\n","    index_src = f'index_text_part_{idx}.pkl'\n","    index_dst = f'gs://{bucket_name}/postings_gcp/text/{index_src}'\n","    !gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":65,"id":"de367d7e","metadata":{},"outputs":[{"data":{"text/plain":["str"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["type(partitioned_dct[0])"]},{"cell_type":"code","execution_count":null,"id":"e714a5aa","metadata":{},"outputs":[],"source":["for idx, dct in enumerate(partitioned_dct):\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix=f\"text/part_{idx}\"):\n","        if not blob.name.endswith(\"pickle\"):\n","            continue\n","        with blob.open(\"rb\") as f:\n","            posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs[k].extend(v)\n","    # Create inverted index instance\n","    inverted_title = InvertedIndex()\n","    # Adding the posting locations dictionary to the inverted index\n","    inverted_title.posting_locs = super_posting_locs\n","    # Add the token - df dictionary to the inverted index\n","    inverted_title.df = title2df_dict\n","    # write the global stats out\n","    inverted_title.write_index('.', f'index_text_part_{idx}', bucket_name)\n","    # upload to gs\n","    index_src = f'index_text_part_{idx}.pkl'\n","    index_dst = f'gs://{bucket_name}/postings_gcp/text/{index_src}'\n","    !gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":49,"id":"17359f59","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/08 07:45:30 WARN YarnAllocator: Container from a bad node: container_1709823484704_0002_01_000009 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:45:30.603]Container killed on request. Exit code is 143\n","[2024-03-08 07:45:30.605]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:45:30.605]Killed by external signal\n",".\n","24/03/08 07:45:30 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 6 for reason Container from a bad node: container_1709823484704_0002_01_000009 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:45:30.603]Container killed on request. Exit code is 143\n","[2024-03-08 07:45:30.605]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:45:30.605]Killed by external signal\n",".\n","24/03/08 07:45:30 ERROR YarnScheduler: Lost executor 6 on cluster-6670-w-1.c.gcp-413611.internal: Container from a bad node: container_1709823484704_0002_01_000009 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:45:30.603]Container killed on request. Exit code is 143\n","[2024-03-08 07:45:30.605]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:45:30.605]Killed by external signal\n",".\n","24/03/08 07:45:30 WARN TaskSetManager: Lost task 21.0 in stage 5.0 (TID 441) (cluster-6670-w-1.c.gcp-413611.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000009 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:45:30.603]Container killed on request. Exit code is 143\n","[2024-03-08 07:45:30.605]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:45:30.605]Killed by external signal\n",".\n","24/03/08 07:45:30 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 446) (cluster-6670-w-1.c.gcp-413611.internal executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000009 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:45:30.603]Container killed on request. Exit code is 143\n","[2024-03-08 07:45:30.605]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:45:30.605]Killed by external signal\n",".\n","24/03/08 07:47:03 WARN YarnAllocator: Container from a bad node: container_1709823484704_0002_01_000013 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:47:03.518]Container killed on request. Exit code is 143\n","[2024-03-08 07:47:03.518]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:47:03.519]Killed by external signal\n",".\n","24/03/08 07:47:03 ERROR YarnScheduler: Lost executor 9 on cluster-6670-w-1.c.gcp-413611.internal: Container from a bad node: container_1709823484704_0002_01_000013 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:47:03.518]Container killed on request. Exit code is 143\n","[2024-03-08 07:47:03.518]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:47:03.519]Killed by external signal\n",".\n","24/03/08 07:47:03 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 9 for reason Container from a bad node: container_1709823484704_0002_01_000013 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:47:03.518]Container killed on request. Exit code is 143\n","[2024-03-08 07:47:03.518]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:47:03.519]Killed by external signal\n",".\n","24/03/08 07:47:03 WARN TaskSetManager: Lost task 1.0 in stage 12.0 (TID 448) (cluster-6670-w-1.c.gcp-413611.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000013 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:47:03.518]Container killed on request. Exit code is 143\n","[2024-03-08 07:47:03.518]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:47:03.519]Killed by external signal\n",".\n","24/03/08 07:47:03 WARN TaskSetManager: Lost task 0.1 in stage 12.0 (TID 447) (cluster-6670-w-1.c.gcp-413611.internal executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000013 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:47:03.518]Container killed on request. Exit code is 143\n","[2024-03-08 07:47:03.518]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:47:03.519]Killed by external signal\n",".\n","24/03/08 07:48:29 WARN YarnAllocator: Container from a bad node: container_1709823484704_0002_01_000014 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:48:29.121]Container killed on request. Exit code is 143\n","[2024-03-08 07:48:29.121]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:48:29.121]Killed by external signal\n",".\n","24/03/08 07:48:29 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 10 for reason Container from a bad node: container_1709823484704_0002_01_000014 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:48:29.121]Container killed on request. Exit code is 143\n","[2024-03-08 07:48:29.121]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:48:29.121]Killed by external signal\n",".\n","24/03/08 07:48:29 ERROR YarnScheduler: Lost executor 10 on cluster-6670-w-1.c.gcp-413611.internal: Container from a bad node: container_1709823484704_0002_01_000014 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:48:29.121]Container killed on request. Exit code is 143\n","[2024-03-08 07:48:29.121]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:48:29.121]Killed by external signal\n",".\n","24/03/08 07:48:29 WARN TaskSetManager: Lost task 1.1 in stage 12.0 (TID 451) (cluster-6670-w-1.c.gcp-413611.internal executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000014 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:48:29.121]Container killed on request. Exit code is 143\n","[2024-03-08 07:48:29.121]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:48:29.121]Killed by external signal\n",".\n","24/03/08 07:48:29 WARN TaskSetManager: Lost task 0.2 in stage 12.0 (TID 450) (cluster-6670-w-1.c.gcp-413611.internal executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000014 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:48:29.121]Container killed on request. Exit code is 143\n","[2024-03-08 07:48:29.121]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:48:29.121]Killed by external signal\n",".\n","24/03/08 07:49:55 WARN YarnAllocator: Container from a bad node: container_1709823484704_0002_01_000015 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:49:55.548]Container killed on request. Exit code is 143\n","[2024-03-08 07:49:55.548]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:49:55.548]Killed by external signal\n",".\n","24/03/08 07:49:55 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 11 for reason Container from a bad node: container_1709823484704_0002_01_000015 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:49:55.548]Container killed on request. Exit code is 143\n","[2024-03-08 07:49:55.548]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:49:55.548]Killed by external signal\n",".\n","24/03/08 07:49:55 ERROR YarnScheduler: Lost executor 11 on cluster-6670-w-1.c.gcp-413611.internal: Container from a bad node: container_1709823484704_0002_01_000015 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:49:55.548]Container killed on request. Exit code is 143\n","[2024-03-08 07:49:55.548]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:49:55.548]Killed by external signal\n",".\n","24/03/08 07:49:55 WARN TaskSetManager: Lost task 1.2 in stage 12.0 (TID 453) (cluster-6670-w-1.c.gcp-413611.internal executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000015 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:49:55.548]Container killed on request. Exit code is 143\n","[2024-03-08 07:49:55.548]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:49:55.548]Killed by external signal\n",".\n","24/03/08 07:49:55 WARN TaskSetManager: Lost task 0.3 in stage 12.0 (TID 452) (cluster-6670-w-1.c.gcp-413611.internal executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000015 on host: cluster-6670-w-1.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 07:49:55.548]Container killed on request. Exit code is 143\n","[2024-03-08 07:49:55.548]Container exited with a non-zero exit code 143. \n","[2024-03-08 07:49:55.548]Killed by external signal\n",".\n","24/03/08 07:49:55 ERROR TaskSetManager: Task 0 in stage 12.0 failed 4 times; aborting job\n","24/03/08 12:53:48 WARN YarnAllocator: Container from a bad node: container_1709823484704_0002_01_000012 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:53:48.750]Container killed on request. Exit code is 143\n","[2024-03-08 12:53:48.750]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:53:48.751]Killed by external signal\n",".\n","24/03/08 12:53:48 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 8 for reason Container from a bad node: container_1709823484704_0002_01_000012 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:53:48.750]Container killed on request. Exit code is 143\n","[2024-03-08 12:53:48.750]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:53:48.751]Killed by external signal\n",".\n","24/03/08 12:53:48 ERROR YarnScheduler: Lost executor 8 on cluster-6670-w-3.c.gcp-413611.internal: Container from a bad node: container_1709823484704_0002_01_000012 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:53:48.750]Container killed on request. Exit code is 143\n","[2024-03-08 12:53:48.750]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:53:48.751]Killed by external signal\n",".\n","24/03/08 12:53:48 WARN TaskSetManager: Lost task 22.0 in stage 9.0 (TID 488) (cluster-6670-w-3.c.gcp-413611.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000012 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:53:48.750]Container killed on request. Exit code is 143\n","[2024-03-08 12:53:48.750]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:53:48.751]Killed by external signal\n",".\n","24/03/08 12:53:48 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 489) (cluster-6670-w-3.c.gcp-413611.internal executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000012 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:53:48.750]Container killed on request. Exit code is 143\n","[2024-03-08 12:53:48.750]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:53:48.751]Killed by external signal\n",".\n","24/03/08 12:54:58 WARN YarnAllocator: Container from a bad node: container_1709823484704_0002_01_000017 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:54:58.189]Container killed on request. Exit code is 143\n","[2024-03-08 12:54:58.190]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:54:58.191]Killed by external signal\n",".\n","24/03/08 12:54:58 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 13 for reason Container from a bad node: container_1709823484704_0002_01_000017 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:54:58.189]Container killed on request. Exit code is 143\n","[2024-03-08 12:54:58.190]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:54:58.191]Killed by external signal\n",".\n","24/03/08 12:54:58 ERROR YarnScheduler: Lost executor 13 on cluster-6670-w-3.c.gcp-413611.internal: Container from a bad node: container_1709823484704_0002_01_000017 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:54:58.189]Container killed on request. Exit code is 143\n","[2024-03-08 12:54:58.190]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:54:58.191]Killed by external signal\n",".\n","24/03/08 12:54:58 WARN TaskSetManager: Lost task 0.1 in stage 6.0 (TID 490) (cluster-6670-w-3.c.gcp-413611.internal executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000017 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:54:58.189]Container killed on request. Exit code is 143\n","[2024-03-08 12:54:58.190]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:54:58.191]Killed by external signal\n",".\n","24/03/08 12:54:58 WARN TaskSetManager: Lost task 1.0 in stage 6.0 (TID 491) (cluster-6670-w-3.c.gcp-413611.internal executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000017 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:54:58.189]Container killed on request. Exit code is 143\n","[2024-03-08 12:54:58.190]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:54:58.191]Killed by external signal\n",".\n","24/03/08 12:56:07 WARN YarnAllocator: Container from a bad node: container_1709823484704_0002_01_000018 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:56:07.387]Container killed on request. Exit code is 143\n","[2024-03-08 12:56:07.387]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:56:07.387]Killed by external signal\n",".\n","24/03/08 12:56:07 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 14 for reason Container from a bad node: container_1709823484704_0002_01_000018 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:56:07.387]Container killed on request. Exit code is 143\n","[2024-03-08 12:56:07.387]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:56:07.387]Killed by external signal\n",".\n","24/03/08 12:56:07 ERROR YarnScheduler: Lost executor 14 on cluster-6670-w-3.c.gcp-413611.internal: Container from a bad node: container_1709823484704_0002_01_000018 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:56:07.387]Container killed on request. Exit code is 143\n","[2024-03-08 12:56:07.387]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:56:07.387]Killed by external signal\n",".\n","24/03/08 12:56:07 WARN TaskSetManager: Lost task 1.1 in stage 6.0 (TID 492) (cluster-6670-w-3.c.gcp-413611.internal executor 14): ExecutorLostFailure (executor 14 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000018 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:56:07.387]Container killed on request. Exit code is 143\n","[2024-03-08 12:56:07.387]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:56:07.387]Killed by external signal\n",".\n","24/03/08 12:56:07 WARN TaskSetManager: Lost task 0.2 in stage 6.0 (TID 493) (cluster-6670-w-3.c.gcp-413611.internal executor 14): ExecutorLostFailure (executor 14 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000018 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:56:07.387]Container killed on request. Exit code is 143\n","[2024-03-08 12:56:07.387]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:56:07.387]Killed by external signal\n",".\n","24/03/08 12:57:19 WARN YarnAllocator: Container from a bad node: container_1709823484704_0002_01_000019 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:57:19.305]Container killed on request. Exit code is 143\n","[2024-03-08 12:57:19.305]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:57:19.305]Killed by external signal\n",".\n","24/03/08 12:57:19 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 15 for reason Container from a bad node: container_1709823484704_0002_01_000019 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:57:19.305]Container killed on request. Exit code is 143\n","[2024-03-08 12:57:19.305]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:57:19.305]Killed by external signal\n",".\n","24/03/08 12:57:19 ERROR YarnScheduler: Lost executor 15 on cluster-6670-w-3.c.gcp-413611.internal: Container from a bad node: container_1709823484704_0002_01_000019 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:57:19.305]Container killed on request. Exit code is 143\n","[2024-03-08 12:57:19.305]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:57:19.305]Killed by external signal\n",".\n","24/03/08 12:57:19 WARN TaskSetManager: Lost task 1.2 in stage 6.0 (TID 495) (cluster-6670-w-3.c.gcp-413611.internal executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000019 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:57:19.305]Container killed on request. Exit code is 143\n","[2024-03-08 12:57:19.305]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:57:19.305]Killed by external signal\n",".\n","24/03/08 12:57:19 WARN TaskSetManager: Lost task 0.3 in stage 6.0 (TID 494) (cluster-6670-w-3.c.gcp-413611.internal executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container from a bad node: container_1709823484704_0002_01_000019 on host: cluster-6670-w-3.c.gcp-413611.internal. Exit status: 143. Diagnostics: [2024-03-08 12:57:19.305]Container killed on request. Exit code is 143\n","[2024-03-08 12:57:19.305]Container exited with a non-zero exit code 143. \n","[2024-03-08 12:57:19.305]Killed by external signal\n",".\n","24/03/08 12:57:19 ERROR TaskSetManager: Task 0 in stage 6.0 failed 4 times; aborting job\n","24/03/08 14:43:39 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 511) (cluster-6670-w-1.c.gcp-413611.internal executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  [Previous line repeated 1 more time]\n","  File \"/tmp/ipykernel_11914/1187013234.py\", line 13, in write_to_bucket\n","  File \"/tmp/ipykernel_11914/427235022.py\", line 127, in partition_postings_and_write\n","AttributeError: 'collections.defaultdict' object has no attribute 'map'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/08 14:45:03 WARN TaskSetManager: Lost task 1.0 in stage 14.0 (TID 512) (cluster-6670-w-1.c.gcp-413611.internal executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  [Previous line repeated 1 more time]\n","  File \"/tmp/ipykernel_11914/1187013234.py\", line 13, in write_to_bucket\n","  File \"/tmp/ipykernel_11914/427235022.py\", line 127, in partition_postings_and_write\n","AttributeError: 'collections.defaultdict' object has no attribute 'map'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/08 14:46:35 WARN TaskSetManager: Lost task 0.1 in stage 14.0 (TID 513) (cluster-6670-w-1.c.gcp-413611.internal executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  [Previous line repeated 1 more time]\n","  File \"/tmp/ipykernel_11914/1187013234.py\", line 13, in write_to_bucket\n","  File \"/tmp/ipykernel_11914/427235022.py\", line 127, in partition_postings_and_write\n","AttributeError: 'collections.defaultdict' object has no attribute 'map'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/08 14:47:56 WARN TaskSetManager: Lost task 1.1 in stage 14.0 (TID 514) (cluster-6670-w-1.c.gcp-413611.internal executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  [Previous line repeated 1 more time]\n","  File \"/tmp/ipykernel_11914/1187013234.py\", line 13, in write_to_bucket\n","  File \"/tmp/ipykernel_11914/427235022.py\", line 127, in partition_postings_and_write\n","AttributeError: 'collections.defaultdict' object has no attribute 'map'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/08 14:49:25 WARN TaskSetManager: Lost task 0.2 in stage 14.0 (TID 515) (cluster-6670-w-1.c.gcp-413611.internal executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  [Previous line repeated 1 more time]\n","  File \"/tmp/ipykernel_11914/1187013234.py\", line 13, in write_to_bucket\n","  File \"/tmp/ipykernel_11914/427235022.py\", line 127, in partition_postings_and_write\n","AttributeError: 'collections.defaultdict' object has no attribute 'map'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/08 14:50:46 WARN TaskSetManager: Lost task 1.2 in stage 14.0 (TID 516) (cluster-6670-w-1.c.gcp-413611.internal executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  [Previous line repeated 1 more time]\n","  File \"/tmp/ipykernel_11914/1187013234.py\", line 13, in write_to_bucket\n","  File \"/tmp/ipykernel_11914/427235022.py\", line 127, in partition_postings_and_write\n","AttributeError: 'collections.defaultdict' object has no attribute 'map'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/08 14:52:17 WARN TaskSetManager: Lost task 0.3 in stage 14.0 (TID 517) (cluster-6670-w-1.c.gcp-413611.internal executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n","    out_iter = func(split_index, iterator)\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n","    return func(split, prev_func(split, iterator))\n","  [Previous line repeated 1 more time]\n","  File \"/tmp/ipykernel_11914/1187013234.py\", line 13, in write_to_bucket\n","  File \"/tmp/ipykernel_11914/427235022.py\", line 127, in partition_postings_and_write\n","AttributeError: 'collections.defaultdict' object has no attribute 'map'\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/08 14:52:17 ERROR TaskSetManager: Task 0 in stage 14.0 failed 4 times; aborting job\n","24/03/08 14:52:17 WARN TaskSetManager: Lost task 1.3 in stage 14.0 (TID 518) (cluster-6670-w-1.c.gcp-413611.internal executor 12): TaskKilled (Stage cancelled)\n"]},{"ename":"Py4JJavaError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 517) (cluster-6670-w-1.c.gcp-413611.internal executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/tmp/ipykernel_11914/1187013234.py\", line 13, in write_to_bucket\n  File \"/tmp/ipykernel_11914/427235022.py\", line 127, in partition_postings_and_write\nAttributeError: 'collections.defaultdict' object has no attribute 'map'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/tmp/ipykernel_11914/1187013234.py\", line 13, in write_to_bucket\n  File \"/tmp/ipykernel_11914/427235022.py\", line 127, in partition_postings_and_write\nAttributeError: 'collections.defaultdict' object has no attribute 'map'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","\u001B[0;32m/tmp/ipykernel_11914/1187013234.py\u001B[0m in \u001B[0;36m<cell line: 39>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[0mgc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m \u001B[0mw2df_part\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitionsWithIndex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwrite_to_bucket\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m","\u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcount\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1519\u001B[0m         \u001B[0;36m3\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1520\u001B[0m         \"\"\"\n\u001B[0;32m-> 1521\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1522\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1523\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mstats\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"RDD[NumberOrArray]\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mStatCounter\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36msum\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1506\u001B[0m         \u001B[0;36m6.0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1507\u001B[0m         \"\"\"\n\u001B[0;32m-> 1508\u001B[0;31m         return self.mapPartitions(lambda x: [sum(x)]).fold(  # type: ignore[return-value]\n\u001B[0m\u001B[1;32m   1509\u001B[0m             \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moperator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1510\u001B[0m         )\n","\u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mfold\u001B[0;34m(self, zeroValue, op)\u001B[0m\n\u001B[1;32m   1334\u001B[0m         \u001B[0;31m# zeroValue provided to each partition is unique from the one provided\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1335\u001B[0m         \u001B[0;31m# to the final reduce call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1336\u001B[0;31m         \u001B[0mvals\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1337\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mzeroValue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1338\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1195\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1197\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1198\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1199\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/opt/conda/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    188\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 190\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    191\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    192\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/opt/conda/miniconda3/lib/python3.10/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 517) (cluster-6670-w-1.c.gcp-413611.internal executor 12): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/tmp/ipykernel_11914/1187013234.py\", line 13, in write_to_bucket\n  File \"/tmp/ipykernel_11914/427235022.py\", line 127, in partition_postings_and_write\nAttributeError: 'collections.defaultdict' object has no attribute 'map'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  [Previous line repeated 1 more time]\n  File \"/tmp/ipykernel_11914/1187013234.py\", line 13, in write_to_bucket\n  File \"/tmp/ipykernel_11914/427235022.py\", line 127, in partition_postings_and_write\nAttributeError: 'collections.defaultdict' object has no attribute 'map'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 7:==============>  (27 + 3) / 32][Stage 9:=============>   (26 + 3) / 32]\r"]}],"source":["import gc\n","def write_to_bucket(idx,rdd):\n","    dct = defaultdict(list)\n","    for key, value in rdd:\n","        dct[key].append(value)\n","    posting_locs_list = partition_postings_and_write(dct).collect()\n","    super_posting_locs = defaultdict(list)\n","    for posting_loc in posting_locs_list:\n","        for k, v in posting_loc.items():\n","            super_posting_locs[k].extend(v)\n","            \n","    # Create the storage client\n","    client = storage.Client()\n","    \n","    # Get the bucket\n","    bucket = client.get_bucket(bucket_name)\n","    \n","    # Define the destination blob\n","    blob_name = f\"index_body_part{idx}.pkl\"  # Adjust this as needed\n","    blob = bucket.blob(f\"postings_gcp/{blob_name}\")\n","    \n","    # Convert the dictionary to bytes using pickle\n","    data_bytes = pickle.dumps(super_posting_locs)\n","    \n","    # Upload the data to the blob\n","    blob.upload_from_string(data_bytes)\n","    \n","    del dct\n","    del super_posting_locs\n","    gc.collect()\n","    return None\n","w2df_part.ForEachPartition(write_to_bucket)"]},{"cell_type":"code","execution_count":null,"id":"d87a29a9","metadata":{},"outputs":[],"source":["word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","postings_filtered = postings.filter(lambda x: len(x[1])>70)\n","w2df = calculate_tf_idf(postings_filtered, total_docs)"]},{"cell_type":"code","execution_count":null,"id":"eb453143","metadata":{},"outputs":[],"source":["w2df_dict = w2df.collectAsMap()\n","_ = partition_postings_and_write(postings_filtered).collect()"]},{"cell_type":"code","execution_count":null,"id":"c8e65e28","metadata":{},"outputs":[],"source":["# Create inverted index instance\n","inv_body = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inv_body.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inv_body.df = w2df_dict\n","# write the global stats out\n","inv_body.write_index('.', 'index')\n","# upload to gs\n","index_src = \"index.pkl\"\n","index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"e0ae41b8","metadata":{},"outputs":[],"source":["title_text = doc_title_pairs.map(lambda y: (y[1], tokenize(y[0]))).collect()"]},{"cell_type":"code","execution_count":null,"id":"fdc935c2","metadata":{},"outputs":[],"source":["body_text = doc_text_pairs.map(lambda y: (y[1], tokenize(y[0]))).collect()"]},{"cell_type":"code","execution_count":null,"id":"5d50b502","metadata":{},"outputs":[],"source":["inv_title = InvertedIndex(dict(title_text))\n","inv_body = InvertedIndex(dict(body_text))"]},{"cell_type":"code","execution_count":null,"id":"16575fa6","metadata":{},"outputs":[],"source":["def get_document_vectors(inverted_index, vocab):\n","    \"\"\"\n","    This function retrieves document vectors from an inverted index with TF-IDF values.\n","\n","    Args:\n","      inverted_index: A dictionary where keys are unique terms and values are postings lists.\n","          Each postings list is a list of tuples (document_id, tfidf_value).\n","\n","    Returns:\n","      A dictionary where keys are document IDs and values are lists representing\n","          sparse document vectors (containing only non-zero TF-IDF values).\n","    \"\"\"\n","    document_vectors = {}\n","    for term, postings_list in inverted_index._posting_list.items():\n","        for document_id, tfidf_value in postings_list:\n","          # Leverage document_id as the index for the sparse vector\n","            sparse_vec_indices = [vocab[term]]\n","            sparse_vec_values = [tfidf_value]\n","            # Update or initialize sparse vector for the document\n","            if document_id in document_vectors:\n","                  document_vectors[document_id].extend(zip(sparse_vec_indices, sparse_vec_values))\n","            else:\n","                  document_vectors[document_id] = list(zip(sparse_vec_indices, sparse_vec_values))\n","    # Convert the lists of indices and values to sparse vectors\n","    document_vectors = {doc_id: Vectors.sparse(len(vocab), [x[0] for x in lst], [x[1] for x in lst]) for doc_id, lst in document_vectors.items()}\n","    return document_vectors\n"]},{"cell_type":"code","execution_count":null,"id":"28e71f41","metadata":{},"outputs":[],"source":["vocab_title = {term:x for x,term in enumerate(inv_title.term_total.keys())}\n","vocab_body = {term:x for x,term in enumerate(inv_body.term_total.keys())}"]},{"cell_type":"code","execution_count":null,"id":"26d43a34","metadata":{},"outputs":[],"source":["doc_vecs_title = get_document_vectors(inv_title, vocab_title)\n","doc_vecs_body = get_document_vectors(inv_body, vocab_body)\n"]},{"cell_type":"code","execution_count":null,"id":"4af27e2e","metadata":{},"outputs":[],"source":["for doc in inv_title.docs:\n","    if doc not in doc_vecs_title.keys():\n","        doc_vecs_title[doc] = Vectors.sparse(len(vocab_title), [])\n","for doc in inv_body.docs:\n","    if doc not in doc_vecs_body.keys():\n","        doc_vecs_body[doc] =  Vectors.sparse(len(vocab_body), [])"]},{"cell_type":"code","execution_count":null,"id":"eedf8411","metadata":{},"outputs":[],"source":["def generate_graph(pages):\n","  ''' Compute the directed graph generated by wiki links.\n","  Parameters:\n","  -----------\n","    pages: RDD\n","      An RDD where each row consists of one wikipedia articles with 'id' and\n","      'anchor_text'.\n","  Returns:\n","  --------\n","    edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the\n","      second entry is the destination page id. No duplicates should be present.\n","    vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph\n","      created by the wikipedia links. No duplicates should be present.\n","  '''\n","  # YOUR CODE HERE\n","  # Convert to a set for faster lookup\n","  links = pages.flatMap(\n","        lambda page: [\n","            (page['id'], dest.id)  # Assuming dest_id is accessible as dest.id\n","            for dest in page['anchor_text']\n","        ]\n","    )\n","\n","  # Remove duplicate links\n","  edges = links.distinct()\n","\n","  # Extract vertices from the links\n","  # Vertices are unique ids present in either the source or destination of a link\n","  vertices = edges.flatMap(lambda link: [link[0], link[1]]).distinct().map(lambda id: (id,))\n","  return edges, vertices"]},{"cell_type":"code","execution_count":null,"id":"5b87d138","metadata":{},"outputs":[],"source":["from graphframes import *"]},{"cell_type":"code","execution_count":null,"id":"3cb4b7bb","metadata":{},"outputs":[],"source":["edges, vertices = generate_graph(doc_anchor_pairs)\n","edgesDF = edges.toDF(['src', 'dst']).repartition(4, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(4, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=10)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('id').asc())"]},{"cell_type":"code","execution_count":null,"id":"2081c4b7","metadata":{},"outputs":[],"source":["from pyspark.ml.linalg import Vectors, VectorUDT\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import DoubleType\n","from pyspark.ml.feature import MinMaxScaler\n","\n","# Define a UDF to convert double to vector\n","to_vector_udf = udf(lambda d: Vectors.dense(d), VectorUDT())\n","\n","# Apply the UDF to create a new column\n","pr = pr.withColumn(\"pagerank_vector\", to_vector_udf(col(\"pagerank\")))\n","\n","# Now you can use the MinMaxScaler\n","scaler = MinMaxScaler(inputCol=\"pagerank_vector\", outputCol=\"pagerank_normalized\")\n","\n","# Fit and transform the scaler\n","scaler_model = scaler.fit(pr)\n","pr_normalized = scaler_model.transform(pr).select(\"id\", \"pagerank_normalized\")\n"]},{"cell_type":"code","execution_count":null,"id":"5aa5d28d","metadata":{},"outputs":[],"source":["rdd = pr_normalized.rdd.map(lambda row: (row['id'], float(np.array(row['pagerank_normalized']))))\n","\n","# Collect the RDD into a dictionary\n","pr_dict = rdd.collectAsMap()"]},{"cell_type":"code","execution_count":null,"id":"748fe52c","metadata":{},"outputs":[],"source":["for doc in inv_title.docs:\n","    if doc not in pr_dict.keys():\n","        pr_dict[doc] = 0"]},{"cell_type":"code","execution_count":null,"id":"95bfa137","metadata":{},"outputs":[],"source":["import numpy as np\n","def search(query):\n","    ''' Returns up to a 100 of your best search results for the query. This is\n","        the place to put forward your best search engine, and you are free to\n","        implement the retrieval whoever you'd like within the bound of the\n","        project requirements (efficiency, quality, etc.). That means it is up to\n","        you to decide on whether to use stemming, remove stopwords, use\n","        PageRank, query expansion, etc.\n","\n","        To issue a query navigate to a URL like:\n","         http://YOUR_SERVER_DOMAIN/search?query=hello+world\n","        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n","        if you're using ngrok on Colab or your external IP on GCP.\n","    Returns:\n","    --------\n","        list of up to 100 search results, ordered from best to worst where each\n","        element is a tuple (wiki_id, title).\n","    '''\n","    # BEGIN SOLUTION\n","#     def load_item_from_pickle(filename):\n","#         with open(filename, 'rb') as file:\n","#           return pickle.load(file)\n","#     title_tf_idf=load_item_from_pickle('title_tf_idf.pkl')\n","#     text_tf_idf=load_item_from_pickle('text_tf_idf.pkl')\n","#     pagerank=load_item_from_pickle('pagerank.pkl')\n","#     allstopwords=load_item_from_pickle('allstopwords.pkl')\n","    def create_sparse_vector_from_counter(query_counter, vocab):\n","\n","        \"\"\"\n","\n","        This function creates a sparse vector from a query represented as a counter.\n","\n","        Args:\n","          query_counter: A Counter object representing the query, where keys are tokens and values are counts.\n","          vocab: A list of unique terms.\n","\n","        Returns:\n","          A sparse vector representing the query, with non-zero counts at indices corresponding to tokens in the vocab.\n","        \"\"\"\n","        # Initialize empty lists for indices and values\n","        sparse_vec_indices = []\n","        sparse_vec_values = []\n","        val_idx = {}\n","        # Iterate over tokens in the query counter\n","        for token, count in query_counter.items():\n","          # Get the index of the token in the vocab list\n","          if token in vocab:\n","            term_index = vocab[token]\n","            # Append the index and count to the sparse vector\n","#             sparse_vec_indices.append(term_index)\n","#             sparse_vec_values.append(count)\n","            val_idx[term_index]=count\n","        # Create a sparse vector from the indices and values\n","        val_idx = dict(sorted(val_idx.items(), key=lambda x: x[0]))\n","        sparse_vector = Vectors.sparse(len(vocab), list(val_idx.keys()), list(val_idx.values()))\n","\n","        return sparse_vector\n","\n","\n","    np.seterr(divide='ignore', invalid='ignore')\n","    stemmer=PorterStemmer()\n","    RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","    tokens = [stemmer.stem(token.group()) if len(token.group()) > 1 else token.group() for token in RE_WORD.finditer(query.lower())]\n","    count = Counter([token for token in tokens])\n","    q_vec_title = create_sparse_vector_from_counter(count,vocab_title)\n","    q_vec_body = create_sparse_vector_from_counter(count,vocab_body)\n","    def cosine_similarity(vec, column_vector):\n","        \"\"\"\n","        Calculate the cosine similarity between two vectors.\n","        \"\"\"\n","        dot_product = vec.dot(column_vector)\n","        norm_vec = vec.norm(2)\n","        norm_column_vector = column_vector.norm(2)\n","        if norm_vec == 0 or norm_column_vector == 0:\n","            similarity = 0\n","        else:\n","            similarity = dot_product / (norm_vec * norm_column_vector)\n","        return similarity\n","    res = []\n","    for title, body in zip(doc_vecs_title.items(),doc_vecs_body.items()):\n","        id = title[0]\n","        vec_title = title[1]\n","        vec_body = body[1]\n","        sim_title = cosine_similarity(q_vec_title,vec_title)\n","        sim_body = cosine_similarity(q_vec_body,vec_body)\n","        pr = pr_dict[id]\n","        res.append((id, sim_title, sim_body, pr))\n","    res = [x if not (np.isnan(x[1]) or np.isnan(x[2])) else (x[0],0.0,0.0, x[3]) for x in res]\n","    return [x[0] for x in sorted(res, key=lambda x: 0.5*x[1]+0.3*x[2]+0.2*x[3], reverse=True)[:100]]"]},{"cell_type":"code","execution_count":null,"id":"68bf7794","metadata":{},"outputs":[],"source":["search(\"fast car\")"]},{"cell_type":"code","execution_count":null,"id":"a55e7e57","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}