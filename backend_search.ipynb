{"cells":[{"cell_type":"code","execution_count":1,"id":"45802994","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","!pip install -q pyspark"]},{"cell_type":"code","execution_count":2,"id":"4b890c7a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["\n","import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from scipy.sparse import dok_matrix\n","from pyspark.ml.linalg import Vectors\n","\n","import math\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')\n"]},{"cell_type":"code","execution_count":3,"id":"1a84bcec","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar 11 15:54 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["\n","# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*\n"]},{"cell_type":"code","execution_count":4,"id":"05481fb6","metadata":{},"outputs":[],"source":["\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *\n"]},{"cell_type":"code","execution_count":5,"id":"d1a06fe0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/03/11 15:57:09 INFO SparkEnv: Registering MapOutputTracker\n","24/03/11 15:57:09 INFO SparkEnv: Registering BlockManagerMaster\n","24/03/11 15:57:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","24/03/11 15:57:09 INFO SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Initialize a Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"YourAppName\") \\\n","    .config(\"spark.driver.memory\", \"32g\") \\\n","    .config(\"spark.executor.memory\", \"5940m\") \\\n","    .config(\"spark.executor.cores\", \"2\") \\\n","    .config(\"spark.driver.maxResultSize\", \"32g\") \\\n","    .getOrCreate()\n"]},{"cell_type":"code","execution_count":6,"id":"6af5cdc5","metadata":{},"outputs":[],"source":["\n","# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'project_479_187' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name[0] == 'm':\n","        paths.append(full_path+b.name)\n"]},{"cell_type":"code","execution_count":156,"id":"a31dcc93","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_title_pairs = parquetFile.select(\"id\", \"title\").rdd"]},{"cell_type":"code","execution_count":8,"id":"a70f42c3","metadata":{},"outputs":[],"source":["doc_title_pairs=doc_title_pairs.repartition(16)\n"]},{"cell_type":"code","execution_count":159,"id":"9842412d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["doc_to_title = {id:(str(id),title) for id,title in doc_title_pairs.collectAsMap().items()}"]},{"cell_type":"code","execution_count":9,"id":"45d1c594","metadata":{},"outputs":[],"source":["class PorterStemmer:\n","    def __init__(self):\n","        self._vowels = {'a', 'e', 'i', 'o', 'u'}\n","        self._step1a_suffixes = [(\"sses\", \"ss\"), (\"ies\", \"i\"), (\"ss\", \"ss\"), (\"s\", \"\")]\n","        self._step1b_suffixes = [(\"eed\", \"ee\"), (\"ed\", \"\"), (\"ing\", \"\")]\n","        self._step2_suffixes = [\n","            (\"ational\", \"ate\"), (\"tional\", \"tion\"), (\"enci\", \"ence\"),\n","            (\"anci\", \"ance\"), (\"izer\", \"ize\"), (\"bli\", \"ble\"),\n","            (\"alli\", \"al\"), (\"entli\", \"ent\"), (\"eli\", \"e\"),\n","            (\"ousli\", \"ous\"), (\"ization\", \"ize\"), (\"ation\", \"ate\"),\n","            (\"ator\", \"ate\"), (\"alism\", \"al\"), (\"iveness\", \"ive\"),\n","            (\"fulness\", \"ful\"), (\"ousness\", \"ous\"), (\"aliti\", \"al\"),\n","            (\"iviti\", \"ive\"), (\"biliti\", \"ble\"), (\"logi\", \"log\")\n","        ]\n","        self._step3_suffixes = [\n","            (\"icate\", \"ic\"), (\"ative\", \"\"), (\"alize\", \"al\"),\n","            (\"iciti\", \"ic\"), (\"ical\", \"\"), (\"ful\", \"\"), (\"ness\", \"\")\n","        ]\n","        self._step4_suffixes = [\n","            (\"al\", \"\"), (\"ance\", \"\"), (\"ence\", \"\"), (\"er\", \"\"),\n","            (\"ic\", \"\"), (\"able\", \"\"), (\"ible\", \"\"), (\"ant\", \"\"),\n","            (\"ement\", \"\"), (\"ment\", \"\"), (\"ent\", \"\"),\n","            (\"ou\", \"\"), (\"ism\", \"\"), (\"ate\", \"\"), (\"iti\", \"\"),\n","            (\"ous\", \"\"), (\"ive\", \"\"), (\"ize\", \"\")\n","        ]\n","        self._step5a_suffixes = [(\"e\", \"\"), (\"e\", \"\")]\n","        self._step5b_suffixes = [(\"ll\", \"l\")]\n","\n","    def _is_consonant(self, word, index):\n","        if index < 0 or index >= len(word):  # Ensure index is within bounds\n","              return False\n","        else:\n","            if word[index] in self._vowels:\n","                  return False\n","            if word[index] == 'y':\n","                  return index == 0 or not self._is_consonant(word, index - 1)\n","        return True\n","\n","\n","    def _measure(self, word):\n","        \"\"\"\n","        Calculate the measure of a word, the number of consonant sequences.\n","        \"\"\"\n","        count = 0\n","        i = 0\n","        while i < len(word):\n","            while i < len(word) and not self._is_consonant(word, i):\n","                i += 1\n","            if i < len(word):\n","                count += 1\n","                while i < len(word) and self._is_consonant(word, i):\n","                    i += 1\n","        return count\n","    def _contains_vowel(self, word):\n","        \"\"\"\n","        Check if a word contains a vowel.\n","        \"\"\"\n","        for char in word:\n","            if char in self._vowels:\n","                return True\n","        return False\n","\n","    def _replace_suffix(self, word, old_suffix, new_suffix):\n","        \"\"\"\n","        Replace the old suffix of a word with a new suffix.\n","        \"\"\"\n","        if word.endswith(old_suffix):\n","            return word[:-len(old_suffix)] + new_suffix\n","        return word\n","\n","    def _apply_rule(self, word, suffixes):\n","        \"\"\"\n","        Apply a list of suffix replacement rules to a word.\n","        \"\"\"\n","        for old_suffix, new_suffix in suffixes:\n","            if word.endswith(old_suffix):\n","                stemmed_word = self._replace_suffix(word, old_suffix, new_suffix)\n","                if self._measure(stemmed_word) > 0:\n","                    return stemmed_word\n","                else:\n","                    return word\n","        return word\n","\n","    def stem(self, word):\n","        \"\"\"\n","        Apply the Porter stemming algorithm to a word.\n","        \"\"\"\n","        if len(word) < 3:\n","            return word\n","\n","        word = self._apply_rule(word, self._step1a_suffixes)\n","\n","        if word.endswith(\"y\") and self._contains_vowel(word[:-1]):\n","            word = word[:-1] + \"i\"\n","\n","        word = self._apply_rule(word, self._step1b_suffixes)\n","\n","        word = self._apply_rule(word, self._step2_suffixes)\n","\n","        word = self._apply_rule(word, self._step3_suffixes)\n","\n","        word = self._apply_rule(word, self._step4_suffixes)\n","\n","        if word.endswith(\"e\"):\n","            stemmed_word = self._replace_suffix(word, \"e\", \"\")\n","            if self._measure(stemmed_word) > 1:\n","                word = stemmed_word\n","            elif self._measure(stemmed_word) == 1 and not self._is_consonant(stemmed_word, -2):\n","                word = stemmed_word\n","\n","        if self._measure(word) > 1 and word.endswith(\"ll\"):\n","            word = word[:-1]\n","\n","        return word\n"]},{"cell_type":"code","execution_count":45,"id":"1dd369da","metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Untitled13.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1-96zO3pwOgfY2ST2rhlwseOZ8R8V2qsR\n","\"\"\"\n","\n","import sys\n","from collections import Counter, OrderedDict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","from time import time\n","from pathlib import Path\n","import pickle\n","from google.cloud import storage\n","from collections import defaultdict\n","from contextlib import closing\n","import math\n","\n","PROJECT_ID = 'YOUR-PROJECT-ID-HERE'\n","def get_bucket(bucket_name):\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","\n","def _open(path, mode, bucket=None):\n","    if bucket is None:\n","        return open(path, mode)\n","    return bucket.blob(path).open(mode)\n","\n","# Let's start with a small block size of 30 bytes just to test things out.\n","BLOCK_SIZE = 1999998\n","\n","class MultiFileWriter:\n","    \"\"\" Sequential binary writer to multiple files of up to BLOCK_SIZE each. \"\"\"\n","    def __init__(self, base_dir, name, bucket_name=None):\n","        self._base_dir = Path(base_dir)\n","        self._name = name\n","        self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","        self._file_gen = (_open(str(self._base_dir / f'{name}_{i:03}.bin'),\n","                                'wb', self._bucket)\n","                          for i in itertools.count())\n","        self._f = next(self._file_gen)\n","\n","    def write(self, b):\n","        locs = []\n","        while len(b) > 0:\n","            pos = self._f.tell()\n","            remaining = BLOCK_SIZE - pos\n","            # if the current file is full, close and open a new one.\n","            if remaining == 0:\n","                self._f.close()\n","                self._f = next(self._file_gen)\n","                pos, remaining = 0, BLOCK_SIZE\n","            self._f.write(b[:remaining])\n","            name = self._f.name if hasattr(self._f, 'name') else self._f._blob.name\n","            locs.append((name, pos))\n","            b = b[remaining:]\n","        return locs\n","\n","    def close(self):\n","        self._f.close()\n","\n","class MultiFileReader:\n","    \"\"\" Sequential binary reader of multiple files of up to BLOCK_SIZE each. \"\"\"\n","    def __init__(self, base_dir, bucket_name=None):\n","        self._base_dir = Path(base_dir)\n","        self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","        self._open_files = {}\n","\n","    def read(self, locs, n_bytes):\n","        b = []\n","        for f_name, offset in locs:\n","            f_name = str(self._base_dir / f_name)\n","            if f_name not in self._open_files:\n","                self._open_files[f_name] = _open(f_name, 'rb', self._bucket)\n","            f = self._open_files[f_name]\n","            f.seek(offset)\n","            n_read = min(n_bytes, BLOCK_SIZE - offset)\n","            b.append(f.read(n_read))\n","            n_bytes -= n_read\n","        return b''.join(b)\n","\n","    def close(self):\n","        for f in self._open_files.values():\n","            f.close()\n","\n","    def __exit__(self, exc_type, exc_value, traceback):\n","        self.close()\n","        return False\n","\n","TUPLE_SIZE = 6       # We're going to pack the doc_id and tf values in this\n","                     # many bytes.\n","TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n","\n","\n","class InvertedIndex:\n","    def __init__(self, docs={}):\n","        \"\"\" Initializes the inverted index and add documents to it (if provided).\n","        Parameters:\n","        -----------\n","          docs: dict mapping doc_id to list of tokens\n","        \"\"\"\n","        # stores document frequency per term\n","        self.df = Counter()\n","        # stores total frequency per term\n","        self.term_total = Counter()\n","        # stores posting list per term while building the index (internally),\n","        # otherwise too big to store in memory.\n","        self._posting_list = defaultdict(list)\n","        # mapping a term to posting file locations, which is a list of\n","        # (file_name, offset) pairs. Since posting lists are big we are going to\n","        # write them to disk and just save their location in this list. We are\n","        # using the MultiFileWriter helper class to write fixed-size files and store\n","        # for each term/posting list its list of locations. The offset represents\n","        # the number of bytes from the beginning of the file where the posting list\n","        # starts.\n","        self.posting_locs = defaultdict(list)\n","        self.n = len(docs)\n","        self.docs=docs.keys()\n","        self.nf={id:len(text) for id, text in docs.items()}\n","\n","        for doc_id, tokens in docs.items():\n","            self.add_doc(doc_id, tokens)\n","\n","    def calc_tf_idf(self, token, tf):\n","        df = self.df[token]\n","        n = self.n\n","        idf = math.log(n/df)\n","        return tf * idf\n","\n","    def add_doc(self, doc_id, tokens):\n","        \"\"\" Adds a document to the index with a given `doc_id` and tokens. It counts\n","            the tf of tokens, then update the index (in memory, no storage\n","            side-effects).\n","        \"\"\"\n","        w2cnt = Counter(tokens)\n","        self.term_total.update(w2cnt)\n","        for w, cnt in w2cnt.items():\n","            self.df[w] = self.df.get(w, 0) + 1\n","            tf_idf = self.calc_tf_idf(w, cnt)\n","            self._posting_list[w].append((doc_id, tf_idf))\n","\n","\n","    def write_index(self, base_dir, name, bucket_name=None):\n","        \"\"\" Write the in-memory index to disk. Results in the file:\n","            (1) `name`.pkl containing the global term stats (e.g. df).\n","        \"\"\"\n","        #### GLOBAL DICTIONARIES ####\n","        self._write_globals(base_dir, name, bucket_name)\n","\n","    def _write_globals(self, base_dir, name, bucket_name):\n","        path = str(Path(base_dir) / f'{name}.pkl')\n","        bucket = None if bucket_name is None else get_bucket(bucket_name)\n","        with _open(path, 'wb', bucket) as f:\n","            pickle.dump(self, f)\n","\n","    def __getstate__(self):\n","        \"\"\" Modify how the object is pickled by removing the internal posting lists\n","            from the object's state dictionary.\n","        \"\"\"\n","        state = self.__dict__.copy()\n","        del state['_posting_list']\n","        return state\n","\n","    def posting_lists_iter(self, base_dir, bucket_name=None):\n","        \"\"\" A generator that reads one posting list from disk and yields\n","            a (word:str, [(doc_id:int, tf:int), ...]) tuple.\n","        \"\"\"\n","        with closing(MultiFileReader(base_dir, bucket_name)) as reader:\n","            for w, locs in self.posting_locs.items():\n","                b = reader.read(locs, self.df[w] * TUPLE_SIZE)\n","                posting_list = []\n","                for i in range(self.df[w]):\n","                    doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","                    tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","                    posting_list.append((doc_id, tf))\n","                yield w, posting_list\n","\n","    def read_a_posting_list(self, base_dir, w, bucket_name=None):\n","        posting_list = []\n","        if not w in self.posting_locs:\n","            return posting_list\n","        with closing(MultiFileReader(base_dir, bucket_name)) as reader:\n","            locs = self.posting_locs[w]\n","            b = reader.read(locs, self.df[w] * TUPLE_SIZE)\n","            for i in range(self.df[w]):\n","                doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","                tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","                posting_list.append((doc_id, tf))\n","        return posting_list\n","\n","    @staticmethod\n","    def write_a_posting_list(b_w_pl, base_dir, bucket_name=None):\n","        posting_locs = defaultdict(list)\n","        bucket_id, list_w_pl = b_w_pl\n","\n","        with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","            for w, pl in list_w_pl:\n","                # convert to bytes\n","                b = b''.join([(doc_id << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')\n","                              for doc_id, tf in pl])\n","                # write to file(s)\n","                locs = writer.write(b)\n","                # save file locations to index\n","                posting_locs[w].extend(locs)\n","            path = str(Path(base_dir) / f'{bucket_id}_posting_locs.pickle')\n","            bucket = None if bucket_name is None else get_bucket(bucket_name)\n","            with _open(path, 'wb', bucket) as f:\n","                pickle.dump(posting_locs, f)\n","        return bucket_id\n","\n","\n","    @staticmethod\n","    def read_index(base_dir, name, bucket_name=None):\n","        path = str(Path(base_dir) / f'{name}.pkl')\n","        bucket = None if bucket_name is None else get_bucket(bucket_name)\n","        with _open(path, 'rb', bucket) as f:\n","            return pickle.load(f)"]},{"cell_type":"code","execution_count":26,"id":"c73398c1","metadata":{},"outputs":[],"source":["import struct\n","\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){0,24}\"\"\", re.UNICODE)\n","def tokenize(text):\n","    stemmer = PorterStemmer()\n","    tokens = [stemmer.stem(token.group()) for token in RE_WORD.finditer(text.lower())]\n","    return [token for token in tokens if token not in all_stopwords]\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","def word_count(text, id):\n","    ''' Count the frequency of each word in text (tf) that is not included in\n","    all_stopwords and return entries that will go into our posting lists.\n","    Parameters:\n","    -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","    Returns:\n","    --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs\n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","    '''\n","    stemmer = PorterStemmer()\n","    tokens = []\n","    alt_tokens = []\n","    for token in RE_WORD.finditer(text.lower()):\n","        tok = token.group()\n","        if tok not in all_stopwords:\n","            st_tok = stemmer.stem(tok)\n","            tokens.append(st_tok)\n","            alt_tokens.append(tok)\n","\n","\n","    count = Counter(tokens)\n","    if len(count)==0:\n","        count = Counter(alt_tokens)\n","    res = []\n","    for tup in count.items():\n","        res.append((tup[0],(id,tup[1])))\n","    return res\n","\n","def reduce_word_counts(unsorted_pl):\n","    ''' Returns a sorted posting list by wiki_id.\n","    Parameters:\n","    -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples\n","    Returns:\n","    --------\n","    list of tuples\n","      A sorted posting list.\n","    '''\n","    # YOUR CODE HERE\n","    sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])\n","    return sorted_pl\n","\n","def calculate_df(postings):\n","    ''' Takes a posting list RDD and calculate the df for each token.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","    '''\n","    # YOUR CODE HERE\n","    return postings.map(lambda x: (x[0], len(x[1])))\n","\n","from math import log\n","\n","def calculate_tf_idf(postings, total_docs):\n","    ''' Calculates the TF-IDF for each token.\n","    Parameters:\n","    -----------\n","      postings: RDD\n","        An RDD where each element is a (token, postings_list) pair, and\n","        postings_list is a list of (document_id, term_frequency) tuples for that token.\n","      total_docs: int\n","        The total number of documents in the corpus.\n","    Returns:\n","    --------\n","      RDD\n","        An RDD where each element is a (token, [(document_id, tf_idf), ...]) pair.\n","    '''\n","    # Calculate IDF for each token\n","    # Note: log base is e by default, which is common in TF-IDF calculations\n","    def idf(df):\n","        return log(total_docs / df)\n","    \n","    # Calculate TF-IDF for each (document_id, term_frequency) tuple in the postings list\n","    def tf_idf(postings_list):\n","        df = len(postings_list)\n","        return [(doc_id, tf * idf(df)) for doc_id, tf in postings_list]\n","    \n","    return postings.map(lambda x: (x[0], tf_idf(x[1])))\n","\n","def partition_postings_and_write(postings):\n","    ''' A function that partitions the posting lists into buckets, writes out\n","    all posting lists in a bucket to disk, and returns the posting locations for\n","    each bucket. Partitioning should be done through the use of `token2bucket`\n","    above. Writing to disk should use the function  `write_a_posting_list`, a\n","    static method implemented in inverted_index_colab.py under the InvertedIndex\n","    class.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and\n","      offsets its posting list was written to. See `write_a_posting_list` for\n","      more details.\n","    '''\n","    # YOUR CODE HERE\n","    buckets = postings.map(lambda x: (token2bucket_id(x[0]),(x[0],x[1]))).groupByKey().mapValues(list)\n","    res = buckets.map(lambda x: InvertedIndex().write_a_posting_list(x,\"text/\", bucket_name))\n","    return res\n","\n","def partition_postings_and_write_title(postings):\n","    ''' A function that partitions the posting lists into buckets, writes out\n","    all posting lists in a bucket to disk, and returns the posting locations for\n","    each bucket. Partitioning should be done through the use of `token2bucket`\n","    above. Writing to disk should use the function  `write_a_posting_list`, a\n","    static method implemented in inverted_index_colab.py under the InvertedIndex\n","    class.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and\n","      offsets its posting list was written to. See `write_a_posting_list` for\n","      more details.\n","    '''\n","    # YOUR CODE HERE\n","    buckets = postings.map(lambda x: (token2bucket_id(x[0]),(x[0],x[1]))).groupByKey().mapValues(list)\n","    res = buckets.map(lambda x: InvertedIndex().write_a_posting_list(x, \"title/\",bucket_name))\n","    return res"]},{"cell_type":"code","execution_count":27,"id":"16575fa6","metadata":{},"outputs":[],"source":["def get_document_vectors(inverted_index, vocab):\n","    \"\"\"\n","    This function retrieves document vectors from an inverted index with TF-IDF values.\n","\n","    Args:\n","      inverted_index: A dictionary where keys are unique terms and values are postings lists.\n","          Each postings list is a list of tuples (document_id, tfidf_value).\n","\n","    Returns:\n","      A dictionary where keys are document IDs and values are lists representing\n","          sparse document vectors (containing only non-zero TF-IDF values).\n","    \"\"\"\n","    document_vectors = {}\n","    for term, postings_list in inverted_index:\n","        for document_id, tfidf_value in postings_list:\n","          # Leverage document_id as the index for the sparse vector\n","            sparse_vec_indices = [vocab[term]]\n","            sparse_vec_values = [tfidf_value]\n","            # Update or initialize sparse vector for the document\n","            if document_id in document_vectors:\n","                  document_vectors[document_id].extend(zip(sparse_vec_indices, sparse_vec_values))\n","            else:\n","                  document_vectors[document_id] = list(zip(sparse_vec_indices, sparse_vec_values))\n","    # Convert the lists of indices and values to sparse vectors\n","    for doc, lst in document_vectors.items():\n","        document_vectors[doc]=sorted(lst, key=lambda x: x[0])\n","    document_vectors = {doc_id: Vectors.sparse(len(vocab), [x[0] for x in lst], [x[1] for x in lst]) for doc_id, lst in document_vectors.items()}\n","    return document_vectors\n"]},{"cell_type":"code","execution_count":28,"id":"c8bdf4ef","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["vocab done\n"]}],"source":["word_counts = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","w2df = calculate_tf_idf(postings, parquetFile.count())\n","vocab = w2df.flatMap(lambda x: [x[0]]).distinct().collect()\n","vocab_dct_title = {term:x for x,term in enumerate(vocab)}\n","\n"]},{"cell_type":"code","execution_count":29,"id":"3ee21372","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["vec done\n"]}],"source":["pst = w2df.collect()"]},{"cell_type":"code","execution_count":41,"id":"faafc12c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["w2dfdict=w2df.collectAsMap()"]},{"cell_type":"code","execution_count":165,"id":"01ff5e94","metadata":{},"outputs":[],"source":["pickle_file_path = 'w2dfdict.pkl'\n","\n","# Save the dictionary as a pickle file in the bucket\n","with open('w2dfdict.pkl', 'wb') as f:\n","    pickle.dump(doc_to_title, f)\n","\n","# Create a blob object for the pickle file\n","blob = bucket.blob(pickle_file_path)\n","\n","# Upload the pickle file to the bucket\n","blob.upload_from_filename('w2dfdict.pkl')"]},{"cell_type":"code","execution_count":47,"id":"e69cfdbe","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":49,"id":"fc9eee8d","metadata":{},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"code","execution_count":168,"id":"95bfa137","metadata":{},"outputs":[],"source":["bucket_name = 'project_479_187' \n","doc_to_title_file_path = 'doc_to_title.pkl'\n","bucket = client.get_bucket(bucket_name)\n","blob_doc_to_title = bucket.blob(doc_to_title_file_path)\n","\n","# Load the pickle file into a dictionary\n","doc_to_title1 = pickle.loads(blob_doc_to_title.download_as_bytes())\n","\n","pst_file_path = 'pst.pkl'\n","blob_pst = bucket.blob(pst_file_path)\n","\n","# Load the pickle file into a dictionary\n","pst = pickle.loads(blob_pst.download_as_bytes())\n","\n","w2dfdict_file_path = 'w2dfdict.pkl'\n","blob_w2dfdict = bucket.blob(w2dfdict_file_path)\n","\n","# Load the pickle file into a dictionary\n","w2dfdict = pickle.loads(blob_w2dfdict.download_as_bytes())\n","\n","import numpy as np\n","import time\n","def search(query):\n","    ''' Returns up to a 100 of your best search results for the query. This is\n","        the place to put forward your best search engine, and you are free to\n","        implement the retrieval whoever you'd like within the bound of the\n","        project requirements (efficiency, quality, etc.). That means it is up to\n","        you to decide on whether to use stemming, remove stopwords, use\n","        PageRank, query expansion, etc.\n","\n","        To issue a query navigate to a URL like:\n","         http://YOUR_SERVER_DOMAIN/search?query=hello+world\n","        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n","        if you're using ngrok on Colab or your external IP on GCP.\n","    Returns:\n","    --------\n","        list of up to 100 search results, ordered from best to worst where each\n","        element is a tuple (wiki_id, title).\n","    '''\n","    # BEGIN SOLUTION\n","#     def load_item_from_pickle(filename):\n","#         with open(filename, 'rb') as file:\n","#           return pickle.load(file)\n","#     title_tf_idf=load_item_from_pickle('title_tf_idf.pkl')\n","#     text_tf_idf=load_item_from_pickle('text_tf_idf.pkl')\n","#     pagerank=load_item_from_pickle('pagerank.pkl')\n","#     allstopwords=load_item_from_pickle('allstopwords.pkl')\n","    def create_sparse_vector_from_counter(query_counter, vocab):\n","\n","        \"\"\"\n","\n","        This function creates a sparse vector from a query represented as a counter.\n","\n","        Args:\n","          query_counter: A Counter object representing the query, where keys are tokens and values are counts.\n","          vocab: A list of unique terms.\n","\n","        Returns:\n","          A sparse vector representing the query, with non-zero counts at indices corresponding to tokens in the vocab.\n","        \"\"\"\n","        # Initialize empty lists for indices and values\n","        sparse_vec_indices = []\n","        sparse_vec_values = []\n","        val_idx = {}\n","        # Iterate over tokens in the query counter\n","        for token, count in query_counter.items():\n","          # Get the index of the token in the vocab list\n","          if token in vocab:\n","            term_index = vocab[token]\n","            # Append the index and count to the sparse vector\n","#             sparse_vec_indices.append(term_index)\n","#             sparse_vec_values.append(count)\n","            val_idx[term_index]=count\n","        # Create a sparse vector from the indices and values\n","        val_idx = dict(sorted(val_idx.items(), key=lambda x: x[0]))\n","        sparse_vector = Vectors.sparse(len(vocab), list(val_idx.keys()), list(val_idx.values()))\n","\n","        return sparse_vector\n","\n","\n","    np.seterr(divide='ignore', invalid='ignore')\n","    stemmer=PorterStemmer()\n","    RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","    tokens = [stemmer.stem(token.group()) if len(token.group()) > 1 else token.group() for token in RE_WORD.finditer(query.lower())]\n","    doc_ids = [doc_id for token in tokens for doc_id, _ in w2dfdict[token] if token in w2dfdict]\n","    count = Counter([token for token in tokens])\n","    q_vec_title = create_sparse_vector_from_counter(count,vocab_dct_title)\n","    # Calculate the cosine similarity between the query vector and the document vectors\n","\n","\n","    stemmer=PorterStemmer()\n","    RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","    tokens = [stemmer.stem(token.group()) if len(token.group()) > 1 else token.group() for token in RE_WORD.finditer(query.lower())]\n","    filtered_vocab = {k: v for k,v in vocab_dct_title.items() if k in tokens}\n","    pst_filtered = [(term, postings_list) for term, postings_list in pst if term in filtered_vocab]\n","    document_vectors = get_document_vectors(pst_filtered, vocab_dct_title)\n","    # Calculate the cosine similarity between the query vector and the document vectors\n","    def cosine_similarity(vec, column_vector):\n","        \"\"\"\n","        Calculate the cosine similarity between two vectors.\n","        \"\"\"\n","        dot_product = vec.dot(column_vector)\n","        norm_vec = vec.norm(2)\n","        norm_column_vector = column_vector.norm(2)\n","        similarity = dot_product / (norm_vec * norm_column_vector)\n","        return similarity\n","\n","    res = []\n","    for doc_id, vec in document_vectors.items():\n","        sim_title = cosine_similarity(q_vec_title, vec)\n","        res.append((doc_id, sim_title))\n","    docs=[x[0] for x in sorted(res, key=lambda x: x[1], reverse=True)[:30]]\n","\n","\n","    # Download the pickle file from the bucket to a local file\n","#     bucket = client.get_bucket(bucket_name)\n","#     blob = bucket.blob(pickle_file_path)\n","#     blob.download_to_filename('doc_to_title.pkl')\n","    values = [doc_to_title1[key] for key in docs if key in doc_to_title1]\n","    return values"]},{"cell_type":"code","execution_count":169,"id":"68bf7794","metadata":{},"outputs":[{"data":{"text/plain":["[('70889', 'Mona Lisa'),\n"," ('168571', 'Mona Lisa (1986 film)'),\n"," ('349909', 'Mona Lisa Overdrive'),\n"," ('883489', 'Mona Lisa Smile'),\n"," ('1197449', '1000 Mona Lisas'),\n"," ('2683924', 'Mona Lisa (opera)'),\n"," ('3378775', 'Mona Lisa (disambiguation)'),\n"," ('4331031', 'Mona Lisa Overdrive (album)'),\n"," ('6185069', 'Isleworth Mona Lisa'),\n"," ('6906354', 'Mona Lisa (singer)'),\n"," ('9587382', 'Mona Lisas and Mad Hatters'),\n"," ('10393560', 'Mona Lisas and Mad Hatters (Part Two)'),\n"," ('12425249', 'The Theft of the Mona Lisa'),\n"," ('13016965', 'I, Mona Lisa'),\n"," ('13893541', 'Mona Lisa (Nat King Cole song)'),\n"," ('14623843', 'Mona Lisa (crater)'),\n"," ('15239160', 'Speculations about Mona Lisa'),\n"," ('16898689', 'Mona Lisa Yuchengco'),\n"," ('16927651', 'Mona Lisa (actress)'),\n"," ('20210450', 'Playing Mona Lisa'),\n"," ('23909451', 'Monday Mona Lisa Club'),\n"," ('26381501', 'Mona Lisa & Others'),\n"," ('29130029', 'Mona Lisa Lost Her Smile'),\n"," ('30512525', 'The Ballad of Mona Lisa'),\n"," ('32363829', 'Mona Lisa (EP)'),\n"," ('36796972', 'Mona Lisa replicas and reinterpretations'),\n"," ('36918311', 'Mona Lisa (band)'),\n"," ('41541874', 'The Mona Lisa (song)'),\n"," ('42884582', 'Mona Lisa Overdrive (song)'),\n"," ('43018414', 'All-Round Appraiser Q: The Eyes of Mona Lisa')]"]},"execution_count":169,"metadata":{},"output_type":"execute_result"}],"source":["search(\"mona lisa\")"]},{"cell_type":"code","execution_count":null,"id":"a55e7e57","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}